{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94333f2f-fb7e-4275-a97f-7ac65207317b",
   "metadata": {},
   "source": [
    "# Deploying Enc-Dec Model with Triton TensorRT-LLM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eadc18-6726-40a9-9375-16b9ac46a0a4",
   "metadata": {},
   "source": [
    "This notebook shows how to optimize Encoder-Decoder aka Seq2Seq Models Like T5/BART using NVIDIA TensorRT-LLM and then deploy them using Triton Inference Server on Amazon SageMaker. TensorRT-LLM library accelerates inference performance on the latest LLMs on NVIDIA GPUs.The Triton Inference Server backend for TensorRT-LLM uses the TensorRT-LLM C++ runtime for highly performant inference execution. It includes techniques like in-flight batching and paged KV caching that provide high throughput at low latency. TensorRT-LLM backend has been bundled with Triton Inference Server and is available as a pre-built container (`xx.yy-trtllm-python-py3`) on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags).\n",
    "\n",
    "**NOTE:** This notebook was tested with the `conda_python3` kernel in `us-east-1` regin on an Amazon SageMaker notebook instance of type `g5.xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25e297-9a33-42bd-a3a6-bab8bc05732e",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "> You can ignore the pip warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ff6d3-5d83-4427-9194-13efcc3753d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU awscli boto3 sagemaker\n",
    "!pip install -q tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b9b89-e8cf-46bb-a10c-ce83287efe95",
   "metadata": {},
   "source": [
    "Also define the SageMaker client and IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton TRT-LLM ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d11bba-59eb-4663-bc55-bbf75882eb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "import numpy as np\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54493f-edec-4747-a1fc-183ece56d3b5",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10311a1-40cc-45bf-956a-a4b6494c0cf5",
   "metadata": {},
   "source": [
    "Next we install `git-lfs` to download Huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e69b79-4505-46dd-9c79-f56f5cf5861f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo amazon-linux-extras install epel -y\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5392c-a06a-4a2e-9f76-10ba9b0bc591",
   "metadata": {},
   "source": [
    "In this example we use `t5-small` model so we download it from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196275b-f5ee-4c43-82b6-e4e77e06246c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"t5-small\"\n",
    "MODEL_TYPE=\"t5\"\n",
    "\n",
    "# For BART\n",
    "# MODEL_NAME=\"bart-base\"\n",
    "# MODEL_TYPE=\"bart\"\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d225c7-02ab-40c7-b2f6-1b9a8064d1de",
   "metadata": {},
   "source": [
    "We download the model from HuggingFace. Or if you have your own trained custom HuggingFace model then you can place it in `workspace/hf_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792608e6-fbc7-4228-bb95-c42bf6bcccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/google-t5/t5-small workspace/hf_models/\n",
    "# For BART use\n",
    "# !git clone git clone https://huggingface.co/facebook/bart-base workspace/hf_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e476fd-4503-4f83-8cd4-86dcc226f99f",
   "metadata": {},
   "source": [
    "## Optimize model with TRT-LLM and Setup Triton Model Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb7572-a57b-42f1-bf5b-b2bab96f21d2",
   "metadata": {},
   "source": [
    "We will be using Triton TRT-LLM NGC container for optimizing our model with Triton TRT-LLM and deployment. So we first pull down the [Triton TRT-LLM image from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags) and then for the sagemaker endpoint deployment we push it to private ECR repo using [push_ecr.sh](./push_ecr.sh) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739ef85-cb20-4d11-bc50-9acf9523c67b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker pull nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3\n",
    "!docker tag nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3 triton-trtllm\n",
    "!bash push_ecr.sh triton-trtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757b359-6faf-43d9-9b49-918bae8322ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_image_uri = f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/triton-trtllm:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a017b1e-b9eb-4036-ba84-2bf4aa1d5ae6",
   "metadata": {},
   "source": [
    "Next we will use the [generate_encdec_triton_model_repo.sh](workspace/generate_encdec_triton_model_repo.sh.sh) script to build the TRT-LLM engine for encoder-decoder T5/BART model and prepare the Triton Model Repository. In this example, we build single-GPU engine (TP Size=1) for T5 model with beam search (max beam width = 2), maximum input len = 1024, maximum output len = 200. To change this, you can edit [generate_encdec_triton_model_repo.sh](workspace/generate_encdec_triton_model_repo.sh) script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c4d80a-5a66-4cb7-b16d-ef5f2c42904d",
   "metadata": {},
   "source": [
    "Next we run [generate_encdec_triton_model_repo.sh](workspace/generate_encdec_triton_model_repo.sh) inside Triton TRT-LLM NGC container. While the docker command below is running feel free to read the cells below for description of what is running in `generate_encdec_triton_model_repo.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6f269-d3b7-45f0-bd0e-2f121bba6b9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus all --ulimit memlock=-1 --shm-size=12g -v ${PWD}/workspace:/workspace \\\n",
    "-w /workspace nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3 \\\n",
    "/bin/bash generate_encdec_triton_model_repo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24324d0-5369-483b-942a-9110af666798",
   "metadata": {},
   "source": [
    "First, we clone the tensorrtllm_backend backend repo which contains example Triton model repository config files under [`all_models/inflight_batcher_llm/`](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main/all_models/inflight_batcher_llm) that we can use. We use ensemble model instead of BLS in this example so we omit copying the `tensorrt_llm_bls` folder. To learn more about ensemble and BLS models, please see the [Ensemble Models](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models) and [Business Logic Scripting documentation](https://github.com/triton-inference-server/python_backend#business-logic-scripting).\n",
    "\n",
    "```\n",
    "git clone https://github.com/triton-inference-server/tensorrtllm_backend.git -b v0.13.0 \n",
    "cd tensorrtllm_backend\n",
    "git lfs install\n",
    "git submodule update --init --recursive\n",
    "cd /workspace\n",
    "\n",
    "rsync -av --exclude='tensorrt_llm_bls' tensorrtllm_backend/all_models/inflight_batcher_llm/ triton_model_repo/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5dc1e0-85fc-4a2a-9330-5be824daf0f8",
   "metadata": {},
   "source": [
    "Next we define the engine building parameters, things like `max_beam_width`, `max_batch_size`, `max_input_len`, `max_output_len`. Here we build TP Size=1 engine for T5 model with beam search (max beam width = 2), maximum input len = 1024, maximum output len = 200. We define the model checkpoint and engine paths and also set them up for sagemaker deployment (with respect to `/opt/ml/model`). \n",
    "\n",
    "```\n",
    "export MODEL_TYPE=\"t5\"\n",
    "export HF_MODEL_PATH=/workspace/hf_models/\n",
    "export UNIFIED_CKPT_PATH=/workspace/ckpt/\n",
    "export ENGINE_PATH=/workspace/triton_model_repo/tensorrt_llm/1/engines/\n",
    "rsync -av ${HF_MODEL_PATH} /workspace/triton_model_repo/tensorrt_llm/1/hf_models/\n",
    "export SAGEMAKER_ENGINE_PATH=/opt/ml/model/tensorrt_llm/1/engines\n",
    "export SAGEMAKER_TOKENIZER_PATH=/opt/ml/model/tensorrt_llm/1/hf_models/\n",
    "INFERENCE_PRECISION=float16\n",
    "TP_SIZE=1\n",
    "MAX_BEAM_WIDTH=2\n",
    "MAX_BATCH_SIZE=8\n",
    "INPUT_LEN=1024\n",
    "OUTPUT_LEN=201\n",
    "```\n",
    "\n",
    "We then convert the Huggingface model checkpoint to TRT-LLM format.\n",
    "\n",
    "```\n",
    "python3 tensorrtllm_backend/tensorrt_llm/examples/enc_dec/convert_checkpoint.py \\\n",
    "--model_type ${MODEL_TYPE} \\\n",
    "--model_dir ${HF_MODEL_PATH} \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--dtype ${INFERENCE_PRECISION} \\\n",
    "--tp_size ${TP_SIZE}\n",
    "```\n",
    "\n",
    "Next we build 2 TRT-LLM engines, one for the T5 encoder, second for the T5 decoder\n",
    "\n",
    "```\n",
    "trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH}/encoder \\\n",
    "--output_dir ${ENGINE_PATH}/encoder \\\n",
    "--kv_cache_type disabled \\\n",
    "--moe_plugin disable \\\n",
    "--enable_xqa disable \\\n",
    "--max_beam_width ${MAX_BEAM_WIDTH} \\\n",
    "--max_input_len ${INPUT_LEN} \\\n",
    "--max_batch_size ${MAX_BATCH_SIZE} \\\n",
    "--gemm_plugin ${INFERENCE_PRECISION} \\\n",
    "--bert_attention_plugin ${INFERENCE_PRECISION} \\\n",
    "--gpt_attention_plugin ${INFERENCE_PRECISION} \\\n",
    "--context_fmha disable # remove for BART\n",
    "\n",
    "echo \"Building TRT-LLM engine for decoder...\"\n",
    "trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH}/decoder \\\n",
    "--output_dir ${ENGINE_PATH}/decoder \\\n",
    "--moe_plugin disable \\\n",
    "--enable_xqa disable \\\n",
    "--max_beam_width ${MAX_BEAM_WIDTH} \\\n",
    "--max_batch_size ${MAX_BATCH_SIZE} \\\n",
    "--gemm_plugin ${INFERENCE_PRECISION} \\\n",
    "--bert_attention_plugin ${INFERENCE_PRECISION} \\\n",
    "--gpt_attention_plugin ${INFERENCE_PRECISION} \\\n",
    "--max_input_len 1 \\\n",
    "--max_encoder_input_len ${INPUT_LEN} \\\n",
    "--max_seq_len ${OUTPUT_LEN} \\\n",
    "--context_fmha disable # remove for BART\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537cb95-3740-4814-98d8-859d783fce56",
   "metadata": {},
   "source": [
    "**Note:** If you want to build multi-GPU engine using Tensor Parallelism then you can set --tp_size in convert_checkpoint.py. For example, for TP=2 on 2-GPU you can set --tp_size=2. If you want to use beam search then set --max_beam_width to higher value than 1. The --max_input_len in encoder trtllm-build controls the model input length and should be same as --max_encoder_input_len in decoder trtllm-build. Additionally, to control the model output len you should set --max_seq_len in decoder trtllm-build to desired output length + 1. It is also advisable to tune --max_num_tokens as the default value of 8192 might be too large or too small depending on your input, output len and use-cases. For BART family models, make sure to remove --context_fmha disable from both encoder and decoder trtllm-build commands. Please refer to [TensorRT-LLM enc-dec example](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/enc_dec#build-tensorrt-engines) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a214e-f766-4c05-b6e7-dcc817d0a73c",
   "metadata": {},
   "source": [
    "Finally, we start preparing our Triton Model Repo `triton_model_repo` by editing the config.pbtxt files. In the directory, there are four subfolders holding artifacts for different parts of the model execution process. The `preprocessing/` and `postprocessing/` folders contain scripts for the Triton Inference Server python backend. These scripts are for tokenizing the text inputs and de-tokenizing the model outputs to convert between strings and the token IDs that the model operates on. These scripts need access to the original huggingface model's tokenizer files which we have placed in `tensorrt_llm/hf_models` in this example.\n",
    "\n",
    "The `tensorrt_llm/engines` folder is where we’ll place the model engines we compiled. And finally, the `ensemble` folder defines a model ensemble that links the previous three components together and tells the Triton Inference Server how to flow data through them. For more details please see [here](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main#prepare-the-model-repository).\n",
    "\n",
    "```\n",
    "python3 tensorrtllm_backend/tools/fill_template.py -i triton_model_repo/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:False, \\\n",
    "max_beam_width:${MAX_BEAM_WIDTH},engine_dir:${SAGEMAKER_ENGINE_PATH}/decoder, \\\n",
    "encoder_engine_dir:${SAGEMAKER_ENGINE_PATH}/encoder,kv_cache_free_gpu_mem_fraction:0.45, \\\n",
    "exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_fused_batching, \\\n",
    "max_queue_delay_microseconds:0,enable_chunked_context:False,max_queue_size:0\n",
    "\n",
    "python3 tensorrtllm_backend/tools/fill_template.py -i triton_model_repo/preprocessing/config.pbtxt tokenizer_dir:${SAGEMAKER_TOKENIZER_PATH},triton_max_batch_size:${MAX_BATCH_SIZE},preprocessing_instance_count:1\n",
    "\n",
    "python3 tensorrtllm_backend/tools/fill_template.py -i triton_model_repo/postprocessing/config.pbtxt tokenizer_dir:${SAGEMAKER_TOKENIZER_PATH},triton_max_batch_size:${MAX_BATCH_SIZE},postprocessing_instance_count:1\n",
    "\n",
    "python3 tensorrtllm_backend/tools/fill_template.py -i triton_model_repo/ensemble/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edadb5-ea1c-4eb8-88bd-0ab1130b5680",
   "metadata": {},
   "source": [
    "Ultimately, after the docker run command for executing the [generate_encdec_triton_model_repo.sh](workspace/generate_encdec_triton_model_repo.sh) script is completed we end up `triton_model_repo` which has directory structure:\n",
    "```\n",
    "triton_model_repo/\n",
    "├── ensemble\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── postprocessing\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "├── preprocessing\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "└── tensorrt_llm\n",
    "    ├── 1\n",
    "    │   ├── engines\n",
    "    │   │   └── t5-small\n",
    "    │   │       ├── decoder\n",
    "    │   │       └── encoder\n",
    "    │   ├── hf_models\n",
    "    │   │   └── t5-small\n",
    "    │   │       ├── config.json\n",
    "    │   │       ├── README.md\n",
    "    │   │       ├── spiece.model\n",
    "    │   │       ├── tokenizer_config.json\n",
    "    │   │       └── tokenizer.json\n",
    "    │   └── model.py\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba30a7-f593-44a4-a71e-6bebccecc21f",
   "metadata": {},
   "source": [
    "## Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091c69c-283d-4947-b30d-d1264d183784",
   "metadata": {},
   "source": [
    "Next, we will package up this Triton model repo `triton_model_repo` in `model.tar.gz` format that SageMaker expects and then upload it to S3 bucket. In this packaging process, we will only retain the tokenizer files from original model checkpoint and exclude any files like `.safetensors`, `.bin`, `.h5`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e254b-aa48-4bfc-9613-63363d8c5ce9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' --exclude='*.bin' \\\n",
    "--exclude='*.h5' --exclude='*.safetensors' --exclude=\"onnx\" \\\n",
    "--exclude='.git*' --exclude='.gitignore' --exclude='.gitattributes' \\\n",
    "--exclude='.gitmodules' --exclude='*.msgpack' --exclude=\"*.ot\" \\\n",
    "-czvf model.tar.gz -C workspace/triton_model_repo/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016845b-a5c9-4eb9-921c-37a41eea3a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-trtllm-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e78ea-2948-4f28-af83-1ac8f191dd55",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e045-f374-4993-b4db-5510aa5ba179",
   "metadata": {},
   "source": [
    "We start off by creating a sagemaker model from the Triton Image Uri and Triton Model Repo we uploaded to S3 in the previous steps.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. In case of ensemble models, this key has to be specified for Triton to startup in SageMaker. We are deploying TRT-LLM ensemble model so we will specify `\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"`\n",
    "\n",
    "Additionally, users can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd82f1-e5bd-46f4-8817-75d5642895e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e00f1-2ff6-482b-b91a-dbfb0d36d4ce",
   "metadata": {},
   "source": [
    "Using the sagemaker model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d0705-09da-4765-a08f-cfe23b954e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039338e-5a5f-4dc4-9fec-c5665fdd09e6",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9f543-8ca2-451b-9b63-dcd5ceea6120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0a77d-94eb-435c-b09a-2bbff3d51b5c",
   "metadata": {},
   "source": [
    "The endpoint creation can take about 10 minutes for the T5 model in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acf053-cf98-4fd5-a9b0-eea10eca3d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02ec7c-41f7-477c-8366-8acaee986703",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "Once we have the endpoint running we can use a sample text to do an inference using json as the payload format. In this example request we are running with `beam search width=2` and requesting TRT-LLM to return `output_log_probs` (Log probabilities for each output) as well as `cum_log_probs`(Cumulative probabilities for each output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4d7a-204f-485f-88a0-235acdda572d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {}\n",
    "text_input = \"translate English to German: How are you?.\"\n",
    "beam_width=2\n",
    "max_tokens=40\n",
    "payload[\"inputs\"] = [{\"name\" : \"text_input\", \"data\" : [text_input], \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "    {\"name\" : \"beam_width\", \"data\" : [beam_width], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]}, \n",
    "    {\"name\" : \"max_tokens\", \"data\" : [max_tokens], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]},\n",
    "    {\"name\" : \"return_log_probs\", \"data\" : [True], \"datatype\" : \"BOOL\", \"shape\" : [1,1]},\n",
    "    ]\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "response_str = response[\"Body\"].read().decode()\n",
    "json_object = json.loads(response_str)\n",
    "json_object['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e7edf-6f58-4709-ae79-816c144b28a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Text Output response from model is\", json_object['outputs'][-1]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221c0f7-95f9-42be-91c4-c62b1e7f6894",
   "metadata": {},
   "source": [
    "We can also send request with other TRT-LLM supported inputs like `temperature`, `repetition_penalty`, `min_length`, `bad_words`, `stop_words`. For more details on TRT-LLM supported input and outputs and how to set them please see [docs here](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/docs/model_config.md#model-input-and-output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571e0e8-d645-4662-9d66-5bf6cda1213b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {}\n",
    "text_input = \"translate English to German: How are you?.\"\n",
    "beam_width=2\n",
    "max_tokens=40\n",
    "temperature=0.9\n",
    "repetition_penalty=1.0\n",
    "min_length=1\n",
    "bad_words=[\"\"]\n",
    "stop_words=[\"\"]\n",
    "\n",
    "payload[\"inputs\"] = [{\"name\" : \"text_input\", \"data\" : [text_input], \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "    {\"name\" : \"beam_width\", \"data\" : [beam_width], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]}, \n",
    "    {\"name\" : \"max_tokens\", \"data\" : [max_tokens], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]},\n",
    "    {\"name\" : \"temperature\", \"data\" : [temperature], \"datatype\" : np_to_triton_dtype(np.float32), \"shape\" : [1,1]},\n",
    "    {\"name\" : \"repetition_penalty\", \"data\" : [repetition_penalty], \"datatype\" : np_to_triton_dtype(np.float32), \"shape\" : [1,1]},\n",
    "    {\"name\" : \"min_length\", \"data\" : [min_length], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]},\n",
    "    {\"name\" : \"bad_words\", \"data\" : bad_words, \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "    {\"name\" : \"stop_words\", \"data\" : stop_words, \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "    {\"name\" : \"return_log_probs\", \"data\" : [True], \"datatype\" : \"BOOL\", \"shape\" : [1,1]},\n",
    "    ]\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "response_str = response[\"Body\"].read().decode()\n",
    "json_object = json.loads(response_str)\n",
    "json_object['outputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db148ac1-5d5b-4d3b-a239-3d084c650392",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0d561-fe15-4c49-9ee8-f408b6b56f43",
   "metadata": {},
   "source": [
    "Once you are done with the endpoint, you can delete it along with other artifacts like sagemaker model and endpoint config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519732f-fcd6-494f-8300-455282e011dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55558df9-df83-44df-ad5d-f6dc600112f9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805dddd-ff22-493f-84f7-3a100cad16db",
   "metadata": {},
   "source": [
    "In this example, you have seen how to optimize encoder-decoder models such as T5 using TRT-LLM and Triton and deploy them on SageMaker. To learn more details about running Encoder-Decoder models with Triton TRT-LLM backend please see [TRT-LLM backend docs](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/docs/encoder_decoder.md). For learning about the best practices for Tuning the Performance of TensorRT-LLM and Triton please see this [guide.](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f1a70-2183-47e6-b5e2-274191078198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
