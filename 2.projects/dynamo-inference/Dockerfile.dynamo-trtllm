# syntax=docker/dockerfile:1.10.0
#
# Dockerfile.dynamo-trtllm-efa - NVIDIA Dynamo with TensorRT-LLM Backend
#
# Features:
# - TensorRT-LLM inference backend for high-performance serving
# - CUDA 12 enforcement (build-time + runtime)
# - AWS EFA support for high-performance networking
# - SSH for distributed training/inference
# - NIXL for accelerated KV cache transfer
#

##############################
# Base Image
##############################
ARG BASE_IMAGE=aws-efa-base-a10:latest
FROM ${BASE_IMAGE}

##############################
# Build ARGs
##############################
ARG NPROC
ARG ARCH="x86_64"

# Versions
ARG DEFAULT_PYTHON_VERSION="3.12"
ARG NIXL_VERSION="0.7.1"
ARG NIXL_GIT_TAG="${NIXL_VERSION}"
ARG DYNAMO_GIT_TAG="main"
ARG RUST_TOOLCHAIN="1.86.0"

##############################
# Path ARGs (from base image)
##############################
ARG CUDA_HOME="/usr/local/cuda"
ARG EFA_PREFIX="/opt/amazon/efa"
ARG GDRCOPY_PREFIX="/opt/gdrcopy"
ARG UCX_PREFIX="/usr/local/ucx"
ARG AWS_OFI_NCCL_PREFIX="/opt/aws-ofi-nccl"
ARG LIBFABRIC_PREFIX="/usr/local"

# NIXL paths
ARG NIXL_PREFIX="/usr/local/nixl"
ARG NIXL_LIB_DIR="${NIXL_PREFIX}/lib/${ARCH}-linux-gnu"
ARG NIXL_PLUGIN_DIR="${NIXL_PREFIX}/lib/${ARCH}-linux-gnu/plugins"

# Python paths
ARG PYTHON_VERSION="3.12"
ARG PYTHON_SITE_PACKAGES="/usr/local/lib/python${PYTHON_VERSION}/dist-packages"

# Application paths
ARG DYNAMO_HOME="/opt/dynamo"
ARG NIXL_BUILD_DIR="/workspace/nixl"

# Rust paths
ARG RUSTUP_HOME="/usr/local/rustup"
ARG CARGO_HOME="/usr/local/cargo"

##############################
# Derived paths
##############################
ARG TORCH_LIB_DIR="${PYTHON_SITE_PACKAGES}/torch/lib"
ARG TENSORRT_LLM_DIR="${PYTHON_SITE_PACKAGES}/tensorrt_llm"
ARG TENSORRT_LLM_LIBS="${TENSORRT_LLM_DIR}/libs"
ARG TENSORRT_DIR="${PYTHON_SITE_PACKAGES}/tensorrt"

##############################
# Environment variables
##############################
ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PIP_BREAK_SYSTEM_PACKAGES=1 \
    # Rust
    RUSTUP_HOME=${RUSTUP_HOME} \
    CARGO_HOME=${CARGO_HOME} \
    # CUDA
    CUDA_HOME=${CUDA_HOME} \
    # Paths from base
    EFA_PATH=${EFA_PREFIX} \
    GDRCOPY_PATH=${GDRCOPY_PREFIX} \
    UCX_PATH=${UCX_PREFIX} \
    # NIXL
    NIXL_PREFIX=${NIXL_PREFIX} \
    NIXL_LIB_DIR=${NIXL_LIB_DIR} \
    NIXL_PLUGIN_DIR=${NIXL_PLUGIN_DIR} \
    # Dynamo
    DYNAMO_HOME=${DYNAMO_HOME} \
    # Python paths for Dynamo
    PYTHONPATH=${DYNAMO_HOME}/components/backends/trtllm/src:${DYNAMO_HOME}/components/frontend/src \
    # System PATH
    PATH=${CARGO_HOME}/bin:${CUDA_HOME}/bin:/usr/local/bin:${PATH} \
    # CUDA Version Enforcement
    REQUIRED_CUDA_MAJOR=12 \
    MIN_DRIVER_VERSION=525

############################
# CUDA 12 Build-time Verification
############################
RUN echo "=== CUDA 12 Build-time Check ===" && \
    NVCC_VERSION=$(nvcc --version | grep "release" | awk '{print $5}' | cut -d',' -f1) && \
    NVCC_MAJOR=$(echo $NVCC_VERSION | cut -d'.' -f1) && \
    echo "NVCC version: $NVCC_VERSION" && \
    if [ "$NVCC_MAJOR" != "12" ]; then \
        echo "ERROR: Base image has CUDA $NVCC_VERSION, requires CUDA 12.x"; \
        exit 1; \
    fi && \
    echo "✅ CUDA 12 base image verified"

############################
# Install system dependencies
############################
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        python3 \
        python3-dev \
        python3-pip \
        python-is-python3 \
        openmpi-bin \
        libopenmpi-dev \
        git \
        build-essential \
        pkg-config \
        libhwloc-dev \
        libudev-dev \
        libclang-dev \
        curl \
        wget \
        cmake \
        protobuf-compiler \
        libprotobuf-dev \
        libzmq5 \
        libzmq3-dev \
        libcpprest-dev \
        libgrpc++-dev \
        libgrpc-dev \
        jq \
        # TensorRT-LLM specific dependencies
        ninja-build \
        patchelf \
    && rm -rf /var/lib/apt/lists/*

############################
# Install OpenSSH for distributed inference
# (Required for multi-node communication)
############################
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openssh-client \
        openssh-server && \
    mkdir -p /var/run/sshd && \
    cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking > /etc/ssh/ssh_config.new && \
    echo "    StrictHostKeyChecking no" >> /etc/ssh/ssh_config.new && \
    mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Configure OpenSSH for MPI
RUN mkdir -p /var/run/sshd && \
    sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd && \
    # Enable root login and pubkey auth
    sed -i 's/#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PubkeyAuthentication.*/PubkeyAuthentication yes/' /etc/ssh/sshd_config && \
    echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "PubkeyAuthentication yes" >> /etc/ssh/sshd_config

# Generate SSH keys for passwordless authentication
RUN rm -rf /root/.ssh/ && \
    mkdir -p /root/.ssh/ && \
    chmod 700 /root/.ssh && \
    ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && \
    cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/id_rsa /root/.ssh/authorized_keys && \
    chmod 644 /root/.ssh/id_rsa.pub && \
    # SSH config to disable host key checking completely
    printf "Host *\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n  LogLevel ERROR\n" > /root/.ssh/config && \
    chmod 600 /root/.ssh/config

############################
# Install Rust toolchain
############################
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \
    sh -s -- -y --no-modify-path --profile minimal --default-toolchain ${RUST_TOOLCHAIN} && \
    chmod -R a+w ${RUSTUP_HOME} ${CARGO_HOME} && \
    rustc --version && cargo --version

############################
# Build NIXL Python bindings
############################
WORKDIR ${NIXL_BUILD_DIR}
RUN rm -rf nixl && \
    git clone --depth 1 --branch "${NIXL_GIT_TAG}" \
        https://github.com/ai-dynamo/nixl.git ${NIXL_BUILD_DIR}

RUN python3 -m pip install --no-cache-dir \
        meson meson-python pybind11 tomlkit && \
    python3 -m pip install --no-cache-dir . && \
    python3 -m pip install --no-cache-dir "nixl==${NIXL_VERSION}"

RUN python3 -c 'import nixl, importlib.metadata as im; \
print("✅ NIXL imported successfully"); \
print("   Version:", im.version("nixl"))'

############################
# Clone Dynamo
############################
RUN git clone https://github.com/ai-dynamo/dynamo.git ${DYNAMO_HOME} && \
    cd ${DYNAMO_HOME} && \
    git checkout "${DYNAMO_GIT_TAG}"

############################
# Install maturin and build Dynamo Rust bindings
############################
RUN python3 -m pip install --no-cache-dir maturin

WORKDIR ${DYNAMO_HOME}/lib/bindings/python
RUN maturin build --release --locked -j ${NPROC:-$(nproc)} && \
    python3 -m pip install --no-cache-dir target/wheels/*.whl

############################
# Install FlashInfer (OPTIONAL - may not have CUDA 12 wheels)
############################
# FlashInfer is optional for TensorRT-LLM - it provides attention optimizations
# but TRT-LLM works without it. As of now, FlashInfer only has wheels for:
# - CUDA 12.1, 12.4, 12.6
# - PyTorch 2.4, 2.5, 2.6
RUN echo "=== Attempting FlashInfer Installation (Optional) ===" && \
    CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $5}' | cut -d',' -f1) && \
    CUDA_MAJOR=$(echo $CUDA_VERSION | cut -d'.' -f1) && \
    CUDA_MINOR=$(echo $CUDA_VERSION | cut -d'.' -f2) && \
    TORCH_VERSION=$(python3 -c "import torch; v=torch.__version__.split('+')[0].split('.'); print(f'{v[0]}{v[1]}')" 2>/dev/null || echo "unknown") && \
    echo "Detected: CUDA ${CUDA_VERSION}, PyTorch version code: ${TORCH_VERSION}" && \
    # Try multiple fallback options
    ( \
        # Option 1: Try exact CUDA version
        python3 -m pip install --no-cache-dir flashinfer-python \
            -i "https://flashinfer.ai/whl/cu${CUDA_MAJOR}${CUDA_MINOR}/torch${TORCH_VERSION}/" 2>/dev/null && \
        echo "✅ FlashInfer installed from cu${CUDA_MAJOR}${CUDA_MINOR}/torch${TORCH_VERSION}" \
    ) || ( \
        # Option 2: Try cu126
        python3 -m pip install --no-cache-dir flashinfer-python \
            -i "https://flashinfer.ai/whl/cu126/torch26/" 2>/dev/null && \
        echo "✅ FlashInfer installed from cu126/torch26 (fallback)" \
    ) || ( \
        # Option 3: Try cu124
        python3 -m pip install --no-cache-dir flashinfer-python \
            -i "https://flashinfer.ai/whl/cu124/torch26/" 2>/dev/null && \
        echo "✅ FlashInfer installed from cu124/torch26 (fallback)" \
    ) || ( \
        # Option 4: Skip FlashInfer entirely
        echo "⚠️  FlashInfer not available for CUDA ${CUDA_VERSION} / PyTorch ${TORCH_VERSION}" && \
        echo "⚠️  TensorRT-LLM will use default attention backend (still functional)" && \
        echo "⚠️  This is OK - FlashInfer is an optional optimization" \
    )

############################
# Install Dynamo with TensorRT-LLM backend
############################
WORKDIR ${DYNAMO_HOME}
RUN echo "=== Installing Dynamo with TensorRT-LLM ===" && \
    # Install tensorrt-llm with all its dependencies
    python3 -m pip install --no-cache-dir \
        --extra-index-url https://pypi.nvidia.com/ \
        tensorrt-llm && \
    # Then install Dynamo
    python3 -m pip install --no-cache-dir \
        --extra-index-url https://pypi.nvidia.com/ \
        -e "." && \
    echo "✅ Dynamo with TensorRT-LLM installed"

############################
# Set LD_LIBRARY_PATH (order matters!)
############################
ENV LD_LIBRARY_PATH="\
${PYTHON_SITE_PACKAGES}/torch/lib:\
${PYTHON_SITE_PACKAGES}/tensorrt_libs:\
${PYTHON_SITE_PACKAGES}/tensorrt_llm/libs:\
${TORCH_LIB_DIR}:\
${TENSORRT_LLM_LIBS}:\
${TENSORRT_DIR}:\
/usr/local/lib:\
${NIXL_LIB_DIR}:\
${NIXL_PLUGIN_DIR}:\
${UCX_PREFIX}/lib:\
${UCX_PREFIX}/lib/ucx:\
${LIBFABRIC_PREFIX}/lib:\
${EFA_PREFIX}/lib:\
${GDRCOPY_PREFIX}/lib64:\
${AWS_OFI_NCCL_PREFIX}/lib:\
${CUDA_HOME}/lib64:\
${LD_LIBRARY_PATH}"

############################
# Validation
############################

# 1. Check CUDA 12 libraries - STRICT CHECK
RUN echo "=== Enforcing CUDA 12 Libraries ===" && \
    ldconfig -p | grep libcublas && \
    if ! ldconfig -p | grep -q "libcublasLt.so.12"; then \
        echo "ERROR: libcublasLt.so.12 NOT FOUND - wrong CUDA version!"; \
        exit 1; \
    fi && \
    if ! ldconfig -p | grep -q "libcudart.so.12"; then \
        echo "ERROR: libcudart.so.12 NOT FOUND - wrong CUDA version!"; \
        exit 1; \
    fi && \
    echo "✅ CUDA 12 libraries verified (libcublasLt.so.12, libcudart.so.12)"

# 2. Verify Python packages - CUDA 12 check
RUN python3 -c "\
import torch; \
import sys; \
import importlib.metadata as im; \
cuda_version = torch.version.cuda; \
cuda_major = int(cuda_version.split('.')[0]); \
print('✅ PyTorch:', torch.__version__, '- CUDA:', cuda_version); \
print('✅ TensorRT-LLM:', im.version('tensorrt-llm')); \
print('✅ NIXL:', im.version('nixl')); \
"

# 3. Verify Dynamo and TensorRT-LLM imports
# Note: tensorrt_llm requires libcuda.so.1 at import time, which is only available
# at runtime with GPU access. We verify the package is installed but skip import.
RUN python3 -c "\
import nixl; \
import dynamo; \
import importlib.metadata as im; \
print('✅ NIXL imported'); \
print('✅ Dynamo imported'); \
print('✅ TensorRT-LLM installed:', im.version('tensorrt-llm')); \
"

# 4. Verify critical shared libraries
RUN python3 -c "\
import ctypes; \
ctypes.CDLL('libpython3.12.so.1.0'); \
ctypes.CDLL('libmpi.so.40'); \
print('✅ libpython3.12.so.1.0 found'); \
print('✅ libmpi.so.40 found'); \
"

# 5. Verify SSH
RUN echo "=== Verifying SSH ===" && \
    test -f /usr/sbin/sshd && echo "✅ sshd found" && \
    test -f /root/.ssh/id_rsa && echo "✅ SSH keys generated"

# 6. Verify library paths
RUN echo "=== Verifying Library Paths ===" && \
    for dir in \
        "${TORCH_LIB_DIR}" \
        "${NIXL_LIB_DIR}" \
        "${UCX_PREFIX}/lib" \
        "${EFA_PREFIX}/lib" \
        "${GDRCOPY_PREFIX}/lib64" \
    ; do \
        if [ -d "$dir" ]; then \
            echo "✅ $dir exists"; \
        else \
            echo "⚠️  $dir not found"; \
        fi; \
    done

# 7. Final summary
RUN echo "========================================" && \
    echo "✅ BUILD COMPLETE - DYNAMO TensorRT-LLM" && \
    echo "========================================" && \
    echo "" && \
    echo "CUDA 12 Enforcement:" && \
    echo "  ✅ Build-time: nvcc version verified" && \
    echo "  ✅ Build-time: libcublasLt.so.12 verified" && \
    echo "  ✅ Build-time: libcudart.so.12 verified" && \
    echo "  ✅ Build-time: PyTorch CUDA 12+ verified" && \
    echo "  ✅ Runtime: Entrypoint checks driver >= 525" && \
    echo "" && \
    echo "Backend: TensorRT-LLM" && \
    echo "" && \
    echo "Key paths:" && \
    echo "  DYNAMO_HOME: ${DYNAMO_HOME}" && \
    echo "  NIXL_PREFIX: ${NIXL_PREFIX}" && \
    echo "  CUDA_HOME: ${CUDA_HOME}" && \
    echo "  EFA_PATH: ${EFA_PREFIX}" && \
    echo "" && \
    python3 -c "import importlib.metadata as im; print(f'  TensorRT-LLM: {im.version(\"tensorrt-llm\")}')" && \
    python3 -c "import importlib.metadata as im; print(f'  NIXL: {im.version(\"nixl\")}')" && \
    echo "" && \
    echo "FlashInfer Status:" && \
    python3 -c "import importlib.metadata as im; print(f'  Installed: {im.version(\"flashinfer-python\")}')" 2>/dev/null || \
    echo "  Not installed (TRT-LLM will use default attention)" && \
    echo "" && \
    echo "Usage:" && \
    echo "  # Start frontend" && \
    echo "  python -m dynamo.frontend --http-port 8000" && \
    echo "" && \
    echo "  # Start TensorRT-LLM worker" && \
    echo "  python -m dynamo.trtllm --model <model-name>"

############################
# Cleanup
############################
RUN python3 -m pip cache purge && \
    rm -rf /root/.cache/pip /tmp/* /var/tmp/* && \
    find ${PYTHON_SITE_PACKAGES} -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true && \
    find ${DYNAMO_HOME} -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true


WORKDIR /workspace
CMD ["/bin/bash"]
