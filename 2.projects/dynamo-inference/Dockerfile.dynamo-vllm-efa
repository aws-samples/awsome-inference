# syntax=docker/dockerfile:1.10.0
#
# Dockerfile.vllm - NVIDIA Dynamo with vLLM Backend
# ==================================================
# Production-ready vLLM with NIXL KV transfer
#
# Stack: CUDA 12.8.1, vLLM 0.11.x, PyTorch 2.7.0a0
#
# Key Features:
# - vLLM with NIXL connector for disaggregated serving
# - CUDA 12.8.1 for compatibility
# - AWS EFA for high-performance networking
# - Disaggregated prefill/decode mode
#
# Usage:
#   docker build --build-arg BASE_IMAGE=dynamo-cuda128-base:latest \
#                -t dynamo-vllm-disagg-nixl:cuda128 \
#                -f Dockerfile.vllm .
#

##############################
# Base Image (CUDA 12.8 EFA)
##############################
ARG BASE_IMAGE=dynamo-cuda128-base:latest
FROM ${BASE_IMAGE}

##############################
# Build ARGs
##############################
ARG NPROC
ARG ARCH="x86_64"

# Versions
ARG DEFAULT_PYTHON_VERSION="3.12"
ARG NIXL_VERSION="0.7.1"
ARG NIXL_GIT_TAG="${NIXL_VERSION}"
ARG DYNAMO_GIT_TAG="v0.7.0"
ARG RUST_TOOLCHAIN="1.86.0"
ARG VLLM_VERSION="0.11.0"

##############################
# Path ARGs
##############################
ARG CUDA_HOME="/usr/local/cuda"
ARG EFA_PREFIX="/opt/amazon/efa"
ARG GDRCOPY_PREFIX="/opt/gdrcopy"
ARG UCX_PREFIX="/usr/local/ucx"
ARG AWS_OFI_NCCL_PREFIX="/opt/aws-ofi-nccl"
ARG LIBFABRIC_PREFIX="/usr/local"

# NIXL paths
ARG NIXL_PREFIX="/usr/local/nixl"
ARG NIXL_LIB_DIR="${NIXL_PREFIX}/lib/${ARCH}-linux-gnu"
ARG NIXL_PLUGIN_DIR="${NIXL_PREFIX}/lib/${ARCH}-linux-gnu/plugins"

# Python paths
ARG PYTHON_VERSION="3.12"
ARG PYTHON_SITE_PACKAGES="/usr/local/lib/python${PYTHON_VERSION}/dist-packages"

# Application paths
ARG DYNAMO_HOME="/opt/dynamo"
ARG NIXL_BUILD_DIR="/workspace/nixl"

# Rust paths
ARG RUSTUP_HOME="/usr/local/rustup"
ARG CARGO_HOME="/usr/local/cargo"

##############################
# Derived paths
##############################
ARG TORCH_LIB_DIR="${PYTHON_SITE_PACKAGES}/torch/lib"
ARG VLLM_DIR="${PYTHON_SITE_PACKAGES}/vllm"

##############################
# Environment variables
##############################
ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PIP_BREAK_SYSTEM_PACKAGES=1 \
    RUSTUP_HOME=${RUSTUP_HOME} \
    CARGO_HOME=${CARGO_HOME} \
    CUDA_HOME=${CUDA_HOME} \
    EFA_PATH=${EFA_PREFIX} \
    GDRCOPY_PATH=${GDRCOPY_PREFIX} \
    UCX_PATH=${UCX_PREFIX} \
    NIXL_PREFIX=${NIXL_PREFIX} \
    NIXL_LIB_DIR=${NIXL_LIB_DIR} \
    NIXL_PLUGIN_DIR=${NIXL_PLUGIN_DIR} \
    DYNAMO_HOME=${DYNAMO_HOME} \
    PYTHONPATH=${DYNAMO_HOME}/components/backends/vllm/src:${DYNAMO_HOME}/components/frontend/src \
    PATH=${CARGO_HOME}/bin:${CUDA_HOME}/bin:/usr/local/bin:${PATH} \
    REQUIRED_CUDA_MAJOR=12 \
    MIN_DRIVER_VERSION=525 \
    # vLLM specific
    VLLM_ATTENTION_BACKEND=FLASHINFER \
    VLLM_USE_V1=1

############################
# CUDA 12.8+ Build-time Verification
############################
RUN echo "=== CUDA 12.8+ Build-time Check ===" && \
    NVCC_VERSION=$(nvcc --version | grep "release" | awk '{print $5}' | cut -d',' -f1) && \
    NVCC_MAJOR=$(echo $NVCC_VERSION | cut -d'.' -f1) && \
    NVCC_MINOR=$(echo $NVCC_VERSION | cut -d'.' -f2) && \
    echo "NVCC version: $NVCC_VERSION" && \
    if [ "$NVCC_MAJOR" != "12" ] || [ "$NVCC_MINOR" -lt "8" ]; then \
        echo "ERROR: Base image has CUDA $NVCC_VERSION, requires CUDA 12.8+"; \
        exit 1; \
    fi && \
    echo "CUDA 12.8+ verified"

############################
# Install additional system dependencies
############################
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openmpi-bin \
        libopenmpi-dev \
        build-essential \
        pkg-config \
        libhwloc-dev \
        libudev-dev \
        cmake \
        protobuf-compiler \
        libprotobuf-dev \
        libzmq5 \
        libzmq3-dev \
        ninja-build \
        patchelf \
        libnuma-dev \
        libucx0 \
    && rm -rf /var/lib/apt/lists/*

############################
# Build NIXL Python bindings
############################
WORKDIR ${NIXL_BUILD_DIR}
RUN rm -rf nixl && \
    git clone --depth 1 --branch "${NIXL_GIT_TAG}" \
        https://github.com/ai-dynamo/nixl.git ${NIXL_BUILD_DIR}

RUN python3 -m pip install --no-cache-dir \
        meson meson-python pybind11 tomlkit && \
    python3 -m pip install --no-cache-dir . && \
    python3 -m pip install --no-cache-dir "nixl==${NIXL_VERSION}"

RUN python3 -c 'import nixl, importlib.metadata as im; \
print("NIXL imported successfully"); \
print("   Version:", im.version("nixl"))'

############################
# Clone Dynamo
############################
RUN git clone https://github.com/ai-dynamo/dynamo.git ${DYNAMO_HOME} && \
    cd ${DYNAMO_HOME} && \
    git checkout "${DYNAMO_GIT_TAG}"

############################
# Install maturin and build Dynamo Rust bindings
############################
RUN python3 -m pip install --no-cache-dir maturin

WORKDIR ${DYNAMO_HOME}/lib/bindings/python
RUN maturin build --release --locked -j ${NPROC:-$(nproc)} && \
    python3 -m pip install --no-cache-dir target/wheels/*.whl

############################
# Install Dynamo with vLLM backend
############################
WORKDIR ${DYNAMO_HOME}
RUN echo "=== Installing Dynamo with vLLM ===" && \
    python3 -m pip install --no-cache-dir \
        --extra-index-url https://pypi.nvidia.com/ \
        -e ".[vllm]" && \
    echo "Dynamo with vLLM installed"

# Verify vLLM installation
RUN python3 -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

############################
# Install FlashInfer for better attention performance
############################
RUN echo "=== Installing FlashInfer ===" && \
    CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $5}' | cut -d',' -f1) && \
    CUDA_MAJOR=$(echo $CUDA_VERSION | cut -d'.' -f1) && \
    CUDA_MINOR=$(echo $CUDA_VERSION | cut -d'.' -f2) && \
    TORCH_VERSION=$(python3 -c "import torch; v=torch.__version__.split('+')[0].split('.'); print(f'{v[0]}{v[1]}')" 2>/dev/null || echo "27") && \
    ( \
        python3 -m pip install --no-cache-dir flashinfer-python \
            -i "https://flashinfer.ai/whl/cu${CUDA_MAJOR}${CUDA_MINOR}/torch${TORCH_VERSION}/" 2>/dev/null && \
        echo "FlashInfer installed" \
    ) || ( \
        python3 -m pip install --no-cache-dir flashinfer-python \
            -i "https://flashinfer.ai/whl/cu126/torch27/" 2>/dev/null && \
        echo "FlashInfer installed (fallback)" \
    ) || ( \
        echo "FlashInfer installation failed - will use default attention backend" \
    )

############################
# Set LD_LIBRARY_PATH
############################
ENV LD_LIBRARY_PATH="\
${PYTHON_SITE_PACKAGES}/torch/lib:\
${TORCH_LIB_DIR}:\
/usr/local/lib:\
${NIXL_LIB_DIR}:\
${NIXL_PLUGIN_DIR}:\
${UCX_PREFIX}/lib:\
${UCX_PREFIX}/lib/ucx:\
${LIBFABRIC_PREFIX}/lib:\
${EFA_PREFIX}/lib:\
${GDRCOPY_PREFIX}/lib64:\
${AWS_OFI_NCCL_PREFIX}/lib:\
${CUDA_HOME}/lib64:\
${LD_LIBRARY_PATH}"

############################
# Validation
############################

# 1. Check CUDA 12 libraries
RUN echo "=== Enforcing CUDA 12 Libraries ===" && \
    ldconfig -p | grep libcublas && \
    if ! ldconfig -p | grep -q "libcublasLt.so.12"; then \
        echo "ERROR: libcublasLt.so.12 NOT FOUND"; \
        exit 1; \
    fi && \
    if ! ldconfig -p | grep -q "libcudart.so.12"; then \
        echo "ERROR: libcudart.so.12 NOT FOUND"; \
        exit 1; \
    fi && \
    echo "CUDA 12 libraries verified"

# 2. Verify Python packages
RUN python3 -c "\
import torch; \
import sys; \
import importlib.metadata as im; \
cuda_version = torch.version.cuda; \
print('PyTorch:', torch.__version__, '- CUDA:', cuda_version); \
print('vLLM:', im.version('vllm')); \
print('NIXL:', im.version('nixl')); \
"

# 3. Verify Dynamo and vLLM imports
RUN python3 -c "\
import nixl; \
import dynamo; \
import vllm; \
print('NIXL imported'); \
print('Dynamo imported'); \
print('vLLM imported'); \
"

# 4. Verify NIXL connector for vLLM
RUN python3 -c "\
try: \
    from vllm.distributed.kv_transfer.kv_connector.v1.nixl_connector import NixlConnector; \
    print('NixlConnector available for vLLM KV transfer'); \
except ImportError as e: \
    print(f'NixlConnector import: {e}'); \
    print('   NIXL connector may need runtime configuration'); \
" || echo "NIXL connector check skipped"

# 5. Final summary
RUN echo "========================================" && \
    echo "BUILD COMPLETE - DYNAMO vLLM" && \
    echo "========================================" && \
    echo "" && \
    echo "Stack: CUDA 12.8.1 + vLLM" && \
    echo "" && \
    echo "Key paths:" && \
    echo "  DYNAMO_HOME: ${DYNAMO_HOME}" && \
    echo "  NIXL_PREFIX: ${NIXL_PREFIX}" && \
    echo "  CUDA_HOME: ${CUDA_HOME}" && \
    echo "" && \
    python3 -c "import importlib.metadata as im; print(f'  vLLM: {im.version(\"vllm\")}')" && \
    python3 -c "import importlib.metadata as im; print(f'  NIXL: {im.version(\"nixl\")}')" && \
    python3 -c "import importlib.metadata as im; print(f'  PyTorch: {im.version(\"torch\")}')"

############################
# Cleanup
############################
RUN python3 -m pip cache purge && \
    rm -rf /root/.cache/pip /tmp/* /var/tmp/* && \
    find ${PYTHON_SITE_PACKAGES} -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true && \
    find ${DYNAMO_HOME} -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true

WORKDIR /workspace
CMD ["/bin/bash"]
