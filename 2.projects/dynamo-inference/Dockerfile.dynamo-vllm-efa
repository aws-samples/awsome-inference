# syntax=docker/dockerfile:1.10.0
#
# Dynamo vLLM Image - AWS EFA Extended
#
# ============================================================================
#
# Build with:
#   docker build -f Dockerfile.vllm \
#     --build-arg DYNAMO_BASE_IMAGE=dynamo-base:latest \
#     --target runtime \
#     -t dynamo-vllm:latest .
#
# Run with (REQUIRED for vLLM):
#   docker run --gpus all \
#     --shm-size=2g \
#     --ulimit memlock=-1 \
#     --ulimit stack=67108864 \
#     --ipc=host \
#     dynamo-vllm:latest

##################################
########## Build Arguments #######
##################################

# Base images
ARG DYNAMO_BASE_IMAGE="dynamo-base:latest"

# ============================================================================
# PyTorch Container Version Configuration
#
# Supported configurations for vLLM:
#
# | vLLM Version | PyTorch Container | CUDA Version | Recommended |
# |--------------|-------------------|--------------|-------------|
# | 0.6.x        | 25.03-py3         | 12.8.1       | Stable      |
# | 0.7.x-0.8.x  | 25.06-py3         | 12.9         | Recommended |
# | latest       | 25.06-py3         | 12.9         | Latest      |
#
# Note: vLLM supports PyTorch 2.4+ with CUDA 12.1+
# ============================================================================
ARG PYTORCH_BASE_IMAGE="nvcr.io/nvidia/pytorch"
ARG PYTORCH_BASE_IMAGE_TAG="25.06-py3"

# Runtime image - MUST match CUDA version from PyTorch base
ARG RUNTIME_IMAGE="nvcr.io/nvidia/cuda-dl-base"
ARG RUNTIME_IMAGE_TAG="25.06-cuda12.9-runtime-ubuntu24.04"

# Architecture
ARG ARCH=amd64
ARG ARCH_ALT=x86_64

# Python
ARG PYTHON_VERSION="3.12"

# ============================================================================
# vLLM Version Configuration
#
# Supported versions (choose one):
#
#   Latest stable release (Default - Recommended):
#     ARG VLLM_VERSION="0.8.0"
#
#   Development version:
#     ARG VLLM_VERSION="main"
#
#   Specific release:
#     ARG VLLM_VERSION="0.7.3"
#
# Installation methods:
#   - PyPI wheel (faster, recommended for releases)
#   - Source build (for custom patches or development)
# ============================================================================
ARG VLLM_VERSION="0.8.0"
ARG VLLM_INSTALL_METHOD="pypi"  # "pypi" or "source"

# FlashInfer version (for optimized attention)
ARG FLASHINFER_VERSION="0.1.6"

# Build parallelism
ARG MAX_JOBS=16

# Optional features
ARG ENABLE_KVBM="false"

# Reference images
FROM ${DYNAMO_BASE_IMAGE} AS dynamo_base
FROM ${PYTORCH_BASE_IMAGE}:${PYTORCH_BASE_IMAGE_TAG} AS pytorch_base

########################################################
########## Framework Build Stage #######################
########################################################
#
# PURPOSE: Build vLLM with PyTorch dependencies
#
# This stage handles vLLM installation which requires:
# - PyTorch from NGC container
# - CUDA development tools for kernel compilation
# - Virtual environment population

FROM ${DYNAMO_BASE_IMAGE} AS framework

ARG ARCH_ALT
ARG PYTHON_VERSION
ARG VLLM_VERSION
ARG VLLM_INSTALL_METHOD
ARG FLASHINFER_VERSION
ARG MAX_JOBS

# Environment setup
ENV NIXL_PREFIX=/opt/nvidia/nvda_nixl
ENV NIXL_LIB_DIR=${NIXL_PREFIX}/lib/${ARCH_ALT}-linux-gnu
ENV NIXL_PLUGIN_DIR=${NIXL_LIB_DIR}/plugins
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"
ENV CUDA_HOME=/usr/local/cuda
ENV MAX_JOBS=${MAX_JOBS}

# Install dependencies for vLLM
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python${PYTHON_VERSION}-dev \
        python3-pip \
        curl \
        git \
        git-lfs \
        cmake \
        ninja-build \
        g++ \
        # RDMA development libraries (for custom transport)
        libibverbs-dev \
        librdmacm-dev \
        libnuma-dev \
        ibverbs-providers \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

# ============================================================================
# PyTorch versions from NGC 25.06 (vLLM compatible)
# These versions are from nvcr.io/nvidia/pytorch:25.06-py3
# ============================================================================
ARG TORCH_VER=2.7.0a0
ARG TORCH_TENSORRT_VER=2.7.0a0
ARG TORCHVISION_VER=0.22.0a0
ARG JINJA2_VER=3.1.6
ARG SYMPY_VER=1.14.0
ARG FLASH_ATTN_VER=2.7.4.post1

# Copy PyTorch and dependencies from NGC PyTorch image
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchgen ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchgen
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision.libs ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision.libs
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/functorch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/functorch
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/jinja2 ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/jinja2
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/jinja2-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/sympy ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/sympy
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/sympy-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/flash_attn
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn_2_cuda.cpython-*-linux-gnu.so ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/

# Clear any existing pip constraints
RUN [ -f /etc/pip/constraint.txt ] && : > /etc/pip/constraint.txt || true && \
    rm -f /etc/apt/sources.list.d/cuda*.list 2>/dev/null || true && \
    rm -f /usr/share/keyrings/cuda-archive-keyring.gpg 2>/dev/null || true

# ============================================================================
# Install FlashInfer (optimized attention kernels)
#
# FlashInfer provides highly optimized attention implementations for vLLM.
# It's optional but highly recommended for better performance.
#
# Reference: https://github.com/flashinfer-ai/flashinfer
# ============================================================================
RUN --mount=type=cache,target=/root/.cache/uv \
    echo "Installing FlashInfer for PyTorch 2.8.0+cu128..." && \
    uv pip install --no-cache flashinfer-python flashinfer-cubin && \
    uv pip install --no-cache flashinfer-jit-cache --index-url https://flashinfer.ai/whl/cu128 && \
    echo "✅ FlashInfer installed" || \
    echo "⚠️ FlashInfer installation failed, vLLM will use fallback attention"

# ============================================================================
# Install vLLM via ai-dynamo[vllm] fork (Dynamo-compatible)
#
# NOTE: Using ai-dynamo[vllm] instead of vanilla vLLM because:
#   - Dynamo 0.7.0 expects the ai-dynamo-vllm fork (targets vLLM 0.10.x)
#   - Fork includes Dynamo-specific patches for KV transfer and NIXL integration
#   - Eliminates version compatibility issues with AsyncMPClient
# ============================================================================
RUN --mount=type=cache,target=/root/.cache/uv \
    echo "Installing ai-dynamo and vLLM ${VLLM_VERSION}..." && \
    uv pip install --no-cache "ai-dynamo" && \
    uv pip install --no-cache "vllm==${VLLM_VERSION}" && \
    echo "✅ ai-dynamo + vLLM ${VLLM_VERSION} installed (FlashInfer 0.5.3 already present)"

# Verify installation
RUN python -c "import torch; print(f'PyTorch: {torch.__version__}')" && \
    python -c "import vllm; print(f'vLLM: {vllm.__version__}')" && \
    echo "✅ Framework dependencies installed"

########################################################
########## Runtime Stage ###############################
########################################################
#
# PURPOSE: Minimal runtime for vLLM inference
#
# IMPORTANT: Run with these Docker flags:
#   --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 --ipc=host

FROM ${RUNTIME_IMAGE}:${RUNTIME_IMAGE_TAG} AS runtime

ARG ARCH_ALT
ARG PYTHON_VERSION
ARG ENABLE_KVBM

# Environment setup
ENV DYNAMO_HOME=/opt/dynamo
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"
ENV NIXL_PREFIX=/opt/nvidia/nvda_nixl
ENV NIXL_LIB_DIR=${NIXL_PREFIX}/lib/${ARCH_ALT}-linux-gnu
ENV NIXL_PLUGIN_DIR=${NIXL_LIB_DIR}/plugins
ENV CUDA_HOME=/usr/local/cuda

WORKDIR /workspace

# Install runtime dependencies
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python${PYTHON_VERSION} \
        python${PYTHON_VERSION}-dev \
        jq \
        git \
        git-lfs \
        curl \
        # JIT compilation support (vLLM compiles kernels at runtime)
        ninja-build \
        g++ \
        # RDMA runtime libraries
        libibverbs1 \
        rdma-core \
        ibverbs-utils \
        libibumad3 \
        libnuma1 \
        librdmacm1 \
        ibverbs-providers \
        libzmq5 \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy CUDA development tools (required for vLLM JIT compilation)
COPY --from=framework /usr/local/cuda/bin/nvcc /usr/local/cuda/bin/nvcc
COPY --from=framework /usr/local/cuda/bin/cudafe++ /usr/local/cuda/bin/cudafe++
COPY --from=framework /usr/local/cuda/bin/ptxas /usr/local/cuda/bin/ptxas
COPY --from=framework /usr/local/cuda/bin/fatbinary /usr/local/cuda/bin/fatbinary
COPY --from=framework /usr/local/cuda/bin/cuda-gdb /usr/local/cuda/bin/cuda-gdb
COPY --from=framework /usr/local/cuda/include/ /usr/local/cuda/include/
COPY --from=framework /usr/local/cuda/nvvm /usr/local/cuda/nvvm
COPY --from=framework /usr/local/cuda/lib64/libcudart.so* /usr/local/cuda/lib64/
COPY --from=framework /usr/local/cuda/lib64/libcublas.so* /usr/local/cuda/lib64/
COPY --from=framework /usr/local/cuda/lib64/libcublasLt.so* /usr/local/cuda/lib64/

# Copy NATS and etcd from base
COPY --from=dynamo_base /usr/bin/nats-server /usr/bin/nats-server
COPY --from=dynamo_base /usr/local/bin/etcd/ /usr/local/bin/etcd/

# Remove HPC-X from pytorch_base to avoid MPI conflicts
RUN rm -rf /opt/hpcx 2>/dev/null || true

# Copy libgomp
COPY --from=framework /usr/lib/${ARCH_ALT}-linux-gnu/libgomp.so* /usr/lib/${ARCH_ALT}-linux-gnu/

# Copy uv
COPY --from=framework /bin/uv /bin/uvx /bin/

# Create dynamo user
RUN userdel -r ubuntu 2>/dev/null || true && \
    useradd -m -s /bin/bash -g 0 dynamo && \
    mkdir -p /home/dynamo/.cache /opt/dynamo && \
    chown -R dynamo: /workspace /home/dynamo /opt/dynamo && \
    chmod -R g+w /workspace /home/dynamo/.cache /opt/dynamo

USER dynamo
ENV HOME=/home/dynamo

# ============================================================================
# Copy EFA components (order matters - base libs first, then plugins)
# ============================================================================

# 1. UCX (base transport)
COPY --chown=dynamo: --from=dynamo_base /usr/local/ucx /usr/local/ucx

# 2. libfabric (EFA provider)
COPY --chown=dynamo: --from=dynamo_base /usr/local/lib/libfabric* /usr/local/lib/

# 3. AWS EFA userspace (BEFORE NCCL plugins)
COPY --chown=dynamo: --from=dynamo_base /opt/amazon /opt/amazon

# 4. GDRCopy
COPY --chown=dynamo: --from=dynamo_base /usr/local/lib/libgdrapi* /usr/local/lib/

# 4.5. System libraries
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libhwloc.so* /usr/lib/x86_64-linux-gnu/
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libevent*.so* /usr/lib/x86_64-linux-gnu/

# 5. NCCL
COPY --from=dynamo_base /usr/local/lib/libnccl.so* /usr/local/lib/
COPY --from=dynamo_base /usr/local/include/nccl*.h /usr/local/include/

# 6. AWS OFI NCCL plugin
COPY --from=dynamo_base /usr/local/lib/libnccl-net* /usr/local/lib/
COPY --from=dynamo_base /usr/local/lib/libnccl-ofi-tuner* /usr/local/lib/

# 7. NIXL
COPY --chown=dynamo: --from=dynamo_base ${NIXL_PREFIX} ${NIXL_PREFIX}

# 8. Validation tools
COPY --from=dynamo_base /opt/nccl-tests /opt/nccl-tests
COPY --from=dynamo_base /usr/local/bin/nixlbench /usr/local/bin/nixlbench

# Refresh library cache
USER root
RUN ldconfig
USER dynamo

# ============================================================================
# Library Path Configuration
#
# The LD_LIBRARY_PATH ordering below ensures correct library resolution:
#   1. Amazon EFA/OpenMPI libraries (required for multi-node communication)
#   2. UCX transport libraries
#   3. NIXL libraries
#   4. CUDA libraries (for JIT compilation)
#   5. PyTorch/vLLM libraries
#
# For AWS EFA configurations, see:
#   https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html
# ============================================================================

ENV PATH="\
/opt/amazon/openmpi/bin:\
/opt/amazon/efa/bin:\
/usr/local/ucx/bin:\
/usr/local/bin/etcd:\
/usr/local/cuda/bin:\
/usr/local/cuda/nvvm/bin:\
${VIRTUAL_ENV}/bin:\
${PATH}"

# Library path priority (highest to lowest):
#   1. Amazon EFA/OpenMPI - Multi-node GPU communication
#   2. UCX - High-performance transport layer
#   3. NIXL - NVIDIA inference exchange library
#   4. System CUDA libraries (for JIT compilation)
ENV LD_LIBRARY_PATH="\
/opt/amazon/openmpi/lib:\
/opt/amazon/efa/lib64:\
/opt/amazon/efa/lib:\
/usr/local/ucx/lib:\
/usr/local/ucx/lib/ucx:\
${NIXL_LIB_DIR}:\
${NIXL_PLUGIN_DIR}:\
/usr/local/lib:\
/usr/local/cuda/lib64:\
${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch/lib:\
${LD_LIBRARY_PATH}"

# C/C++ include path for JIT compilation
ENV CPATH=/usr/local/cuda/include:${CPATH}

# Configure Amazon OpenMPI
ENV MPI_HOME=/opt/amazon/openmpi
ENV OPAL_PREFIX=
ENV HPCX_VERSION=
ENV OMPI_MCA_plm_rsh_agent=/bin/false

# AWS EFA environment (graceful degradation)
ENV FI_EFA_FORK_SAFE=1
ENV FI_LOG_LEVEL=warn
ENV FI_PROVIDER=efa

# UCX environment
ENV UCX_TLS=self,sm
ENV UCX_NET_DEVICES=all

# vLLM specific settings
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn
ENV VLLM_ATTENTION_BACKEND=FLASHINFER

# Copy virtual environment with vLLM
COPY --chown=dynamo: --from=framework ${VIRTUAL_ENV} ${VIRTUAL_ENV}

# Install Dynamo and NIXL wheels
COPY --chown=dynamo: --from=dynamo_base /opt/dynamo/wheelhouse/ /opt/dynamo/wheelhouse/

# ============================================================
# Package Installation Options
#
# Option 1 (Default): Install from PyPI
#   - ai-dynamo installed from public PyPI repository
#   - NIXL installed from local wheelhouse
#
# Option 2: Install from local wheelhouse (if available)
#   Uncomment and modify the RUN command below to use:
#   uv pip install --no-cache /opt/dynamo/wheelhouse/ai_dynamo*.whl
#
# Option 3: Install specific version
#   uv pip install --no-cache ai-dynamo==<version>
# ============================================================
RUN --mount=type=cache,target=/home/dynamo/.cache/uv,uid=1000 \
    uv pip install --no-cache ai-dynamo && \
    uv pip install --no-cache /opt/dynamo/wheelhouse/nixl/*.whl && \
    if [ "${ENABLE_KVBM}" = "true" ] && ls /opt/dynamo/wheelhouse/kvbm*.whl 1>/dev/null 2>&1; then \
        uv pip install --no-cache /opt/dynamo/wheelhouse/kvbm*.whl; \
    fi

# Verify installation
RUN python -c "import dynamo; print('✅ Dynamo orchestration')" && \
    python -c "import vllm; print(f'✅ vLLM {vllm.__version__}')" && \
    python -c "import nixl; print(f'✅ NIXL {nixl.__version__}')" 2>/dev/null || true

# Setup bash environment
USER root
RUN echo 'source /opt/dynamo/venv/bin/activate' >> /etc/bash.bashrc && \
    echo '# Dynamo vLLM Runtime' >> /etc/bash.bashrc && \
    echo '# Run with: --shm-size=2g --ulimit memlock=-1 --ipc=host' >> /etc/bash.bashrc

USER dynamo

ENTRYPOINT ["/bin/bash", "-c"]
CMD ["bash"]

########################################################
########## Development Stage ###########################
########################################################

FROM runtime AS dev

USER root

RUN apt-get update -y && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        nvtop wget tmux vim iproute2 rsync zip unzip htop \
        autoconf automake cmake libtool meson net-tools \
        pybind11-dev clang libclang-dev protobuf-compiler && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

COPY --from=dynamo_base /usr/local/rustup /usr/local/rustup
COPY --from=dynamo_base /usr/local/cargo /usr/local/cargo

ENV RUSTUP_HOME=/usr/local/rustup
ENV CARGO_HOME=/usr/local/cargo
ENV PATH="/usr/local/cargo/bin:${PATH}"
ENV CARGO_TARGET_DIR=/workspace/target
ENV DYNAMO_HOME=/workspace

RUN uv pip install --no-cache maturin[patchelf]

CMD ["bash"]
