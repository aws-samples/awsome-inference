# Benchmark Configuration Parameters
# Define parameters for different benchmark types

# vLLM Throughput Benchmark Configuration
throughput:
  batch_sizes: [1, 2, 4, 8, 16, 32, 64]
  input_lengths: [128, 512, 1024, 8192, 32768, 102400]
  output_lengths: [100, 500, 1000, 2000]
  num_prompts: 100
  warmup_runs: 10
  dataset: "random"
  ignore_eos: true
  stream: false
  percentiles: [25, 50, 90, 95, 99]
  metrics:
    - "request_throughput"
    - "output_token_throughput"
    - "input_token_throughput"
    - "gpu_utilization"
    - "memory_usage"

# vLLM Latency Benchmark Configuration
latency:
  concurrency_levels: [1, 2, 4, 8, 16, 32, 48, 64]
  input_length: 102400
  output_length: 500
  num_prompts: 100
  warmup_runs: 10
  dataset: "random"
  ignore_eos: true
  stream: false
  percentiles: [25, 50, 90, 95, 99]
  metrics:
    - "ttft"  # Time to First Token
    - "tpot"  # Time Per Output Token
    - "itl"   # Inter-Token Latency
    - "e2el"  # End-to-End Latency

# GenAI-Perf Standard Benchmark Configuration
genai_perf:
  concurrency: 16
  num_prompts: 330
  warmup_requests: 10
  request_count: 320
  input_tokens_mean: 102400
  input_tokens_stddev: 0
  output_tokens_mean: 500
  output_tokens_stddev: 500
  measurement_interval: 300000
  endpoint_type: "chat"
  random_seed: 0
  generate_plots: true
  extra_inputs:
    min_tokens: 500
    max_tokens: 1000
    ignore_eos: true
  metrics:
    - "ttft_p50"
    - "ttft_p90"
    - "ttft_p99"
    - "itl_p50"
    - "itl_p90"
    - "itl_p99"
    - "request_throughput"
    - "output_token_throughput"

# GenAI-Perf Parameter Sweep Configuration
genai_perf_sweep:
  concurrency_levels: [1, 2, 4, 8, 16, 32, 64]
  input_token_configs:
    - mean: 128
      stddev: 0
    - mean: 1024
      stddev: 0
    - mean: 8192
      stddev: 0
    - mean: 32768
      stddev: 0
    - mean: 102400
      stddev: 0
  output_token_configs:
    - mean: 100
      stddev: 50
    - mean: 500
      stddev: 250
    - mean: 1000
      stddev: 500
    - mean: 2000
      stddev: 1000
  num_prompts: 330
  warmup_requests: 10

# Multi-GPU Benchmark Configuration
multi_gpu:
  gpu_counts: [1, 2, 4, 8]
  input_length: 102400
  output_length: 500
  batch_sizes: [1, 2, 4, 8, 16, 32]
  num_prompts: 100
  warmup_runs: 10
  metrics:
    - "throughput_per_gpu"
    - "scaling_efficiency"
    - "gpu_utilization"
    - "memory_per_gpu"

# Tensor Parallel Benchmark Configuration
tensor_parallel:
  tp_sizes: [1, 2, 4, 8]
  input_length: 102400
  output_length: 500
  batch_size: 16
  num_prompts: 100
  warmup_runs: 10
  metrics:
    - "throughput"
    - "latency"
    - "communication_overhead"
    - "scaling_efficiency"

# Pipeline Parallel Benchmark Configuration
pipeline_parallel:
  pp_sizes: [1, 2, 4]
  tp_sizes: [1, 2, 4, 8]
  input_length: 102400
  output_length: 500
  batch_size: 16
  num_prompts: 100
  warmup_runs: 10
  metrics:
    - "throughput"
    - "latency"
    - "pipeline_bubble"
    - "scaling_efficiency"

# Quantization Benchmark Configuration
quantization:
  quantization_levels:
    - "fp16"    # Baseline
    - "int8"    # INT8 quantization
    - "int4"    # INT4 quantization
    - "fp8"     # FP8 (H100 only)
    - "awq"     # AWQ quantization
    - "gptq"    # GPTQ quantization
  input_length: 102400
  output_length: 500
  batch_size: 16
  concurrency: 16
  num_prompts: 100
  warmup_runs: 10
  metrics:
    - "throughput"
    - "latency"
    - "memory_usage"
    - "quality_degradation"
    - "speedup_vs_fp16"

# Prefill vs Decode Benchmark Configuration
prefill_decode:
  input_lengths: [128, 512, 1024, 8192, 32768, 102400]
  output_lengths: [100, 500, 1000, 2000]
  batch_sizes: [1, 4, 16, 32]
  num_prompts: 100
  warmup_runs: 10
  disaggregated_mode: false
  metrics:
    - "prefill_latency"
    - "prefill_throughput"
    - "decode_latency"
    - "decode_throughput"
    - "phase_ratio"
    - "prefill_gpu_utilization"
    - "decode_gpu_utilization"

# Token Generation Benchmark Configuration
token_generation:
  output_lengths: [100, 500, 1000, 2000, 5000]
  input_length: 102400
  batch_sizes: [1, 4, 16, 32]
  streaming_modes: [true, false]
  temperature_values: [0.0, 0.7, 1.0, 1.5]
  top_p_values: [0.9, 0.95, 1.0]
  num_prompts: 100
  warmup_runs: 10
  metrics:
    - "tokens_per_second"
    - "generation_latency"
    - "streaming_overhead"
    - "quality_variance"

# End-to-End Integration Test Configuration
e2e_integration:
  scenarios:
    - name: "low_concurrency"
      concurrency: 4
      input_length: 8192
      output_length: 500
      duration: 300  # seconds
    - name: "medium_concurrency"
      concurrency: 16
      input_length: 32768
      output_length: 500
      duration: 600
    - name: "high_concurrency"
      concurrency: 64
      input_length: 102400
      output_length: 500
      duration: 900
    - name: "burst_load"
      concurrency: 128
      input_length: 8192
      output_length: 1000
      duration: 300
  warmup_duration: 60
  metrics:
    - "sustained_throughput"
    - "p99_latency"
    - "error_rate"
    - "gpu_utilization"
    - "memory_usage"

# Performance Targets by GPU Type
performance_targets:
  h100_8x:
    llama_70b:
      ttft_p99_ms: 1500
      itl_p50_ms: 50
      throughput_rps: 20
      gpu_utilization: 0.85
    llama_13b:
      ttft_p99_ms: 800
      itl_p50_ms: 30
      throughput_rps: 50
      gpu_utilization: 0.80
    llama_7b:
      ttft_p99_ms: 400
      itl_p50_ms: 20
      throughput_rps: 100
      gpu_utilization: 0.75

  a100_8x:
    llama_70b:
      ttft_p99_ms: 2500
      itl_p50_ms: 80
      throughput_rps: 12
      gpu_utilization: 0.80
    llama_13b:
      ttft_p99_ms: 1200
      itl_p50_ms: 50
      throughput_rps: 30
      gpu_utilization: 0.75
    llama_7b:
      ttft_p99_ms: 600
      itl_p50_ms: 30
      throughput_rps: 60
      gpu_utilization: 0.70

  a10g_single:
    llama_7b:
      ttft_p99_ms: 500
      itl_p50_ms: 30
      throughput_rps: 15
      gpu_utilization: 0.75

# Result Collection Configuration
results:
  output_dir: "/home/ubuntu/awsome-inference/2.projects/dynamo-inference/benchmarks/vllm-genai-perf/results"
  format: "json"
  save_raw_data: true
  save_plots: true
  compression: true
  retention_days: 90
  fields:
    - "timestamp"
    - "benchmark_type"
    - "model"
    - "configuration"
    - "metrics"
    - "system_info"
    - "errors"

# Report Generation Configuration
reporting:
  formats: ["html", "pdf", "csv", "json"]
  include_plots: true
  include_raw_data: false
  comparison_enabled: true
  historical_analysis: true
  alert_on_regression: true
  regression_threshold: 0.10  # 10% degradation

# Global Settings
global:
  timeout_seconds: 3600
  retry_on_failure: true
  max_retries: 3
  cleanup_on_error: true
  verbose_logging: true
  log_level: "INFO"
  save_logs: true
  log_dir: "/home/ubuntu/awsome-inference/2.projects/dynamo-inference/benchmarks/vllm-genai-perf/logs"
