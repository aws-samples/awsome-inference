# GPU and Parallelism Configuration
# Define GPU configurations for different deployment scenarios

# Single GPU Configurations
single_gpu:
  h100_single:
    description: "Single H100 80GB GPU"
    gpu_count: 1
    tensor_parallel: 1
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-7b"
      - "llama-7b-instruct"
      - "llama-13b"
      - "llama-13b-instruct"

  a100_single:
    description: "Single A100 80GB GPU"
    gpu_count: 1
    tensor_parallel: 1
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    compute_capability: "8.0"
    supports_fp8: false
    recommended_models:
      - "llama-7b"
      - "llama-7b-instruct"
      - "llama-13b"
      - "llama-13b-instruct"

  a10g_single:
    description: "Single A10G 24GB GPU"
    gpu_count: 1
    tensor_parallel: 1
    pipeline_parallel: 1
    memory_per_gpu: "24GB"
    compute_capability: "8.6"
    supports_fp8: false
    recommended_models:
      - "llama-7b"
      - "llama-7b-instruct"

# Multi-GPU Configurations - 2 GPUs
multi_gpu_2x:
  h100_2x_tp:
    description: "2x H100 with Tensor Parallelism"
    gpu_count: 2
    tensor_parallel: 2
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "160GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-13b"
      - "llama-13b-instruct"
      - "llama-70b"

  a100_2x_tp:
    description: "2x A100 with Tensor Parallelism"
    gpu_count: 2
    tensor_parallel: 2
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "160GB"
    compute_capability: "8.0"
    supports_fp8: false
    recommended_models:
      - "llama-13b"
      - "llama-13b-instruct"

# Multi-GPU Configurations - 4 GPUs
multi_gpu_4x:
  h100_4x_tp:
    description: "4x H100 with Tensor Parallelism"
    gpu_count: 4
    tensor_parallel: 4
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "320GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  a100_4x_tp:
    description: "4x A100 with Tensor Parallelism"
    gpu_count: 4
    tensor_parallel: 4
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "320GB"
    compute_capability: "8.0"
    supports_fp8: false
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"

  h100_4x_pp2:
    description: "4x H100 with Pipeline Parallelism (PP=2, TP=2)"
    gpu_count: 4
    tensor_parallel: 2
    pipeline_parallel: 2
    memory_per_gpu: "80GB"
    total_memory: "320GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"

# Multi-GPU Configurations - 8 GPUs
multi_gpu_8x:
  h100_8x_tp:
    description: "8x H100 with Tensor Parallelism"
    gpu_count: 8
    tensor_parallel: 8
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "640GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  a100_8x_tp:
    description: "8x A100 with Tensor Parallelism"
    gpu_count: 8
    tensor_parallel: 8
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "640GB"
    compute_capability: "8.0"
    supports_fp8: false
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  h100_8x_pp2:
    description: "8x H100 with Pipeline Parallelism (PP=2, TP=4)"
    gpu_count: 8
    tensor_parallel: 4
    pipeline_parallel: 2
    memory_per_gpu: "80GB"
    total_memory: "640GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  h100_8x_pp4:
    description: "8x H100 with Pipeline Parallelism (PP=4, TP=2)"
    gpu_count: 8
    tensor_parallel: 2
    pipeline_parallel: 4
    memory_per_gpu: "80GB"
    total_memory: "640GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"

# Multi-GPU Configurations - 16 GPUs
multi_gpu_16x:
  h100_16x_tp:
    description: "16x H100 with Tensor Parallelism"
    gpu_count: 16
    tensor_parallel: 16
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "1280GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  h100_16x_tp8_pp2:
    description: "16x H100 with TP=8, PP=2"
    gpu_count: 16
    tensor_parallel: 8
    pipeline_parallel: 2
    memory_per_gpu: "80GB"
    total_memory: "1280GB"
    compute_capability: "9.0"
    supports_fp8: true
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

  a100_16x_tp:
    description: "16x A100 with Tensor Parallelism"
    gpu_count: 16
    tensor_parallel: 16
    pipeline_parallel: 1
    memory_per_gpu: "80GB"
    total_memory: "1280GB"
    compute_capability: "8.0"
    supports_fp8: false
    recommended_models:
      - "llama-70b"
      - "llama-70b-instruct"
      - "llama-3.3-70b"

# Disaggregated Deployment Configurations
disaggregated:
  prefill_decode_2_6:
    description: "2 GPUs for Prefill, 6 GPUs for Decode"
    total_gpus: 8
    prefill_gpus: 2
    decode_gpus: 6
    prefill_tensor_parallel: 2
    decode_tensor_parallel: 6
    compute_capability: "9.0"
    supports_fp8: true
    recommended_for:
      - "High throughput scenarios"
      - "Long context prefill with fast decode"

  prefill_decode_4_4:
    description: "4 GPUs for Prefill, 4 GPUs for Decode"
    total_gpus: 8
    prefill_gpus: 4
    decode_gpus: 4
    prefill_tensor_parallel: 4
    decode_tensor_parallel: 4
    compute_capability: "9.0"
    supports_fp8: true
    recommended_for:
      - "Balanced workloads"
      - "Medium context length"

  prefill_decode_1_7:
    description: "1 GPU for Prefill, 7 GPUs for Decode"
    total_gpus: 8
    prefill_gpus: 1
    decode_gpus: 7
    prefill_tensor_parallel: 1
    decode_tensor_parallel: 7
    compute_capability: "9.0"
    supports_fp8: true
    recommended_for:
      - "Very high decode throughput"
      - "Short prompts with long outputs"

# NCCL and Network Configurations
network_configs:
  nvlink_only:
    description: "NVLink only (single node)"
    communication_backend: "nccl"
    nccl_settings:
      NCCL_IB_DISABLE: "1"
      NCCL_P2P_LEVEL: "NVL"
      NCCL_SHM_DISABLE: "0"
    recommended_for: "Single node, multi-GPU"

  nvlink_ib:
    description: "NVLink + InfiniBand (multi-node)"
    communication_backend: "nccl"
    nccl_settings:
      NCCL_IB_DISABLE: "0"
      NCCL_IB_HCA: "mlx5_0,mlx5_1,mlx5_2,mlx5_3"
      NCCL_IB_GID_INDEX: "3"
      NCCL_NET_GDR_LEVEL: "5"
    recommended_for: "Multi-node deployments"

  efa:
    description: "AWS EFA (Elastic Fabric Adapter)"
    communication_backend: "nccl"
    nccl_settings:
      NCCL_SOCKET_IFNAME: "ens"
      FI_PROVIDER: "efa"
      FI_EFA_USE_DEVICE_RDMA: "1"
      NCCL_PROTO: "simple"
    recommended_for: "AWS P5/P4d instances"

  roce:
    description: "RoCE (RDMA over Converged Ethernet)"
    communication_backend: "nccl"
    nccl_settings:
      NCCL_IB_DISABLE: "0"
      NCCL_IB_HCA: "=eth"
      NCCL_IB_GID_INDEX: "3"
      NCCL_NET_GDR_LEVEL: "5"
    recommended_for: "RoCE-enabled networks"

# Memory Optimization Configurations
memory_configs:
  maximum_throughput:
    description: "Optimize for maximum throughput"
    gpu_memory_utilization: 0.95
    kv_cache_dtype: "auto"
    max_num_seqs: 128
    enable_prefix_caching: true
    enable_chunked_prefill: false
    block_size: 16

  low_latency:
    description: "Optimize for low latency"
    gpu_memory_utilization: 0.80
    kv_cache_dtype: "auto"
    max_num_seqs: 32
    enable_prefix_caching: false
    enable_chunked_prefill: true
    block_size: 8

  balanced:
    description: "Balanced throughput and latency"
    gpu_memory_utilization: 0.90
    kv_cache_dtype: "fp8"
    max_num_seqs: 64
    enable_prefix_caching: true
    enable_chunked_prefill: false
    block_size: 16

  memory_constrained:
    description: "For memory-constrained scenarios"
    gpu_memory_utilization: 0.85
    kv_cache_dtype: "fp8"
    max_num_seqs: 32
    enable_prefix_caching: false
    enable_chunked_prefill: true
    block_size: 8

# AWS Instance Type Mappings
aws_instances:
  p5_48xlarge:
    description: "AWS P5 (8x H100)"
    instance_type: "p5.48xlarge"
    gpu_type: "h100"
    gpu_count: 8
    gpu_memory_per_gpu: "80GB"
    network: "efa"
    recommended_config: "h100_8x_tp"

  p4d_24xlarge:
    description: "AWS P4d (8x A100)"
    instance_type: "p4d.24xlarge"
    gpu_type: "a100"
    gpu_count: 8
    gpu_memory_per_gpu: "40GB"
    network: "efa"
    recommended_config: "a100_8x_tp"

  p4de_24xlarge:
    description: "AWS P4de (8x A100 80GB)"
    instance_type: "p4de.24xlarge"
    gpu_type: "a100"
    gpu_count: 8
    gpu_memory_per_gpu: "80GB"
    network: "efa"
    recommended_config: "a100_8x_tp"

  g5_12xlarge:
    description: "AWS G5 (4x A10G)"
    instance_type: "g5.12xlarge"
    gpu_type: "a10g"
    gpu_count: 4
    gpu_memory_per_gpu: "24GB"
    network: "ena"
    recommended_config: "a10g_single"

  g5_48xlarge:
    description: "AWS G5 (8x A10G)"
    instance_type: "g5.48xlarge"
    gpu_type: "a10g"
    gpu_count: 8
    gpu_memory_per_gpu: "24GB"
    network: "ena"
    recommended_config: "a10g_single"

# Default GPU Configuration
default:
  gpu_count: 1
  tensor_parallel: 1
  pipeline_parallel: 1
  gpu_memory_utilization: 0.90
  max_num_seqs: 64
  enable_prefix_caching: false
  enable_chunked_prefill: false
