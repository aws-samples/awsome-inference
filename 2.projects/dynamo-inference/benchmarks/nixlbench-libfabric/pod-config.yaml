# NIXL Benchmark Pod Configuration - LIBFABRIC Backend
# Optimized for 46+ GB/s GPU-to-GPU RDMA performance
#
# This configuration includes:
# - Anti-affinity to ensure cross-node placement
# - EFA device allocation (vpc.amazonaws.com/efa)
# - GPU allocation (nvidia.com/gpu)
# - HugePages for optimal RDMA performance
# - Privileged mode for GPU Direct RDMA
# - Host networking for low-latency communication
#
# Deploy both pods together:
#   kubectl apply -f pod-config.yaml
#
# Verify pod placement (must be on different nodes):
#   kubectl get pods -o wide | grep nixl-bench

---
apiVersion: v1
kind: Pod
metadata:
  name: nixl-bench-node1
  labels:
    app: nixl-benchmark
    role: node1
    benchmark-group: nixl-efa-test
spec:
  restartPolicy: Always
  hostNetwork: true
  hostIPC: true

  # Anti-affinity to ensure pods are on different physical nodes
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: benchmark-group
            operator: In
            values:
            - nixl-efa-test
        topologyKey: kubernetes.io/hostname

  containers:
  - name: nixl-benchmark
    image: 058264135704.dkr.ecr.us-east-2.amazonaws.com/nixl-aligned:0.7.1-bench
    imagePullPolicy: IfNotPresent
    command: ["/bin/bash", "-c", "sleep infinity"]

    securityContext:
      capabilities:
        add:
        - IPC_LOCK       # Required for RDMA memory locking
        - SYS_PTRACE     # Required for debugging
      privileged: true   # Required for GPU Direct RDMA

    resources:
      requests:
        nvidia.com/gpu: 1
        vpc.amazonaws.com/efa: 1    # EFA device allocation
        memory: 32Gi
        cpu: 8
        hugepages-2Mi: 2Gi          # HugePages for RDMA
      limits:
        nvidia.com/gpu: 1
        vpc.amazonaws.com/efa: 1
        memory: 32Gi
        cpu: 8
        hugepages-2Mi: 2Gi

    volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: hugepages
      mountPath: /dev/hugepages

    env:
    # Pre-configure LIBFABRIC environment variables
    - name: FI_PROVIDER
      value: "efa"                    # Use EFA provider
    - name: FI_EFA_ENABLE_SHM_TRANSFER
      value: "0"                      # Disable SHM for cross-node
    - name: NCCL_DEBUG
      value: "INFO"                   # Enable NCCL logging

  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 16Gi                 # Shared memory for RDMA
  - name: hugepages
    emptyDir:
      medium: HugePages

---
apiVersion: v1
kind: Pod
metadata:
  name: nixl-bench-node2
  labels:
    app: nixl-benchmark
    role: node2
    benchmark-group: nixl-efa-test
spec:
  restartPolicy: Always
  hostNetwork: true
  hostIPC: true

  # Anti-affinity to ensure pods are on different physical nodes
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: benchmark-group
            operator: In
            values:
            - nixl-efa-test
        topologyKey: kubernetes.io/hostname

  containers:
  - name: nixl-benchmark
    image: 058264135704.dkr.ecr.us-east-2.amazonaws.com/nixl-aligned:0.7.1-bench
    imagePullPolicy: IfNotPresent
    command: ["/bin/bash", "-c", "sleep infinity"]

    securityContext:
      capabilities:
        add:
        - IPC_LOCK       # Required for RDMA memory locking
        - SYS_PTRACE     # Required for debugging
      privileged: true   # Required for GPU Direct RDMA

    resources:
      requests:
        nvidia.com/gpu: 1
        vpc.amazonaws.com/efa: 1    # EFA device allocation
        memory: 32Gi
        cpu: 8
        hugepages-2Mi: 2Gi          # HugePages for RDMA
      limits:
        nvidia.com/gpu: 1
        vpc.amazonaws.com/efa: 1
        memory: 32Gi
        cpu: 8
        hugepages-2Mi: 2Gi

    volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: hugepages
      mountPath: /dev/hugepages

    env:
    # Pre-configure LIBFABRIC environment variables
    - name: FI_PROVIDER
      value: "efa"                    # Use EFA provider
    - name: FI_EFA_ENABLE_SHM_TRANSFER
      value: "0"                      # Disable SHM for cross-node
    - name: NCCL_DEBUG
      value: "INFO"                   # Enable NCCL logging

  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 16Gi                 # Shared memory for RDMA
  - name: hugepages
    emptyDir:
      medium: HugePages

---
# Notes:
#
# 1. Anti-Affinity:
#    The benchmark-group label ensures pods are scheduled on different physical nodes.
#    This is REQUIRED for cross-node RDMA testing.
#
# 2. EFA Allocation:
#    vpc.amazonaws.com/efa: 1 requests one EFA device per pod.
#    Your cluster must have the EFA device plugin installed.
#
# 3. GPU Allocation:
#    nvidia.com/gpu: 1 requests one GPU per pod.
#    Required for VRAM-to-VRAM (GPU Direct) transfers.
#
# 4. HugePages:
#    hugepages-2Mi: 2Gi allocates 2GB of HugePages for RDMA optimization.
#    Your nodes must have HugePages enabled and available.
#
# 5. Privileged Mode:
#    privileged: true is REQUIRED for GPU Direct RDMA (P2P).
#    This allows direct access to GPU memory from the network adapter.
#
# 6. Host Networking:
#    hostNetwork: true provides low-latency networking.
#    Pods share the host's network namespace.
#
# 7. Image:
#    Uses nixl-aligned:0.7.1-bench with EFA, CUDA, and libfabric pre-installed.
#    Replace with your own image URI if needed.
#
# 8. Verification:
#    After deployment, verify:
#    - kubectl get pods -o wide | grep nixl-bench  (check different nodes)
#    - kubectl describe pod nixl-bench-node1 | grep -E "efa|gpu"  (check resources)
#    - kubectl exec nixl-bench-node1 -- ls /dev/infiniband  (check EFA devices)
#    - kubectl exec nixl-bench-node1 -- nvidia-smi -L  (check GPUs)
