# Credits

## Contributors

This project was developed with contributions from:

- **Anton Alexander** - Sr. Specialist, WW Foundation Models
- **Alex Iankoulski** - Principal WW Specialist SA GenAI

## Acknowledgments

This project builds upon and integrates the following components:

### NVIDIA Components
- **CUDA** and **cuDNN** - GPU acceleration frameworks (Apache-2.0)
- **NCCL** - Collective communications library (BSD-3-Clause)
- **TensorRT-LLM** - High-performance LLM inference engine
- **Triton Inference Server** - Model serving framework
- **NVIDIA Dynamo** - Distributed inference orchestration

### Networking and Communication
- **libfabric** - Fabric communication library (BSD/GPL)
- **AWS EFA** - Elastic Fabric Adapter for high-performance networking
- **UCX** - Unified Communication X framework (BSD-3-Clause)
- **NIXL** - Network Infrastructure for eXascale Learning

### Machine Learning Frameworks
- **vLLM** - LLM inference and serving engine (Apache-2.0)
- **PyTorch** - Machine learning framework (BSD-3-Clause)
- **Hugging Face Transformers** - Model repository and utilities (Apache-2.0)

### Infrastructure
- **Kubernetes** - Container orchestration
- **AWS SageMaker HyperPod** - Managed ML infrastructure
- **Docker** - Container runtime

## License

This project is licensed under MIT-0. See the repository LICENSE file for details.

Individual components may be licensed under different terms. Please refer to the respective component documentation for license information.
