# syntax=docker/dockerfile:1.10.0
#
# TensorRT-LLM Production Image - CUDA 12.8.1 with DeepGEMM Enabled (FIXED)
# =========================================================================
# Stack B: CUDA 12.8.1, TRT-LLM 0.20.x, PyTorch 2.7.0a0
# Fix: MPI/HPC-X libevent conflict resolved
#

ARG BASE_IMAGE=public.ecr.aws/hpc-cloud/efa:a10g
FROM ${BASE_IMAGE}

ARG NPROC=8
ARG NIXL_VERSION="0.7.1"
ARG DYNAMO_GIT_TAG="v0.7.0"
ARG TRTLLM_VERSION="0.20.0"
ARG RUST_TOOLCHAIN="1.86.0"

ENV RUSTUP_HOME=/usr/local/rustup \
    CARGO_HOME=/usr/local/cargo \
    PATH=/usr/local/cargo/bin:${PATH} \
    DYNAMO_HOME=/opt/dynamo \
    PYTHONPATH=/opt/dynamo/components/backends/trtllm/src:/opt/dynamo/components/frontend/src

############################
# CUDA 12.8+ verification
############################
RUN echo "=== TRT-LLM Build: Verifying CUDA 12.8+ ===" && \
    nvcc --version | grep "12.8" && \
    echo "CUDA 12.8 confirmed - DeepGEMM will be enabled"

############################
# Install system dependencies
# IMPORTANT: Do NOT install openmpi-bin/libopenmpi-dev from apt
# They conflict with HPC-X OpenMPI causing opal_libevent2022 errors
############################
RUN apt-get update && apt-get install -y --no-install-recommends \
    protobuf-compiler libprotobuf-dev \
    libzmq5 libzmq3-dev libcpprest-dev libgrpc++-dev libgrpc-dev \
    ninja-build patchelf libclang-dev \
    && rm -rf /var/lib/apt/lists/*

############################
# Ensure Amazon OpenMPI is used, not HPC-X
# This prevents the libevent2022 symbol conflict
############################
ENV OMPI_DIR=/opt/amazon/openmpi \
    PATH=/opt/amazon/openmpi/bin:${PATH} \
    LD_LIBRARY_PATH=/opt/amazon/openmpi/lib:${LD_LIBRARY_PATH}

# Remove HPC-X from MCA paths to prevent loading conflicting modules
ENV OMPI_MCA_mca_base_component_path=/opt/amazon/openmpi/lib/openmpi

############################
# Install Rust
############################
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \
    sh -s -- -y --no-modify-path --profile minimal --default-toolchain ${RUST_TOOLCHAIN} && \
    rustc --version && cargo --version

############################
# Clone Dynamo
############################
RUN git clone https://github.com/ai-dynamo/dynamo.git ${DYNAMO_HOME} && \
    cd ${DYNAMO_HOME} && git checkout "${DYNAMO_GIT_TAG}"

############################
# Build Dynamo Rust bindings
############################
RUN pip install --no-cache-dir maturin

WORKDIR ${DYNAMO_HOME}/lib/bindings/python
RUN maturin build --release --locked -j ${NPROC} && \
    pip install --no-cache-dir target/wheels/*.whl

############################
# Install FlashInfer (CUDA 12.8)
############################
RUN pip install --no-cache-dir flashinfer-python \
    -i "https://flashinfer.ai/whl/cu128/torch27/" 2>/dev/null || \
    pip install --no-cache-dir flashinfer-python \
    -i "https://flashinfer.ai/whl/cu126/torch27/" 2>/dev/null || \
    echo "FlashInfer not available - will use default attention"

############################
# Install TensorRT-LLM 0.20.x (requires TensorRT 10.10)
############################
WORKDIR ${DYNAMO_HOME}

# Remove the NGC constraint file entirely to allow TRT-LLM 0.20 to install
RUN echo "=== Removing NGC pip constraints ===" && \
    echo "" > /etc/pip/constraint.txt && \
    echo "NGC constraints removed"

# Upgrade TensorRT to match TRT-LLM 0.20 requirements
RUN echo "=== Upgrading TensorRT for TRT-LLM 0.20 ===" && \
    pip uninstall -y tensorrt tensorrt-cu12 tensorrt-cu12-bindings tensorrt-cu12-libs 2>/dev/null || true && \
    pip install --no-cache-dir \
        --index-url https://pypi.nvidia.com/ \
        --extra-index-url https://pypi.org/simple/ \
        "tensorrt~=10.10.0" && \
    echo "TensorRT upgraded"

RUN echo "=== Installing TensorRT-LLM ${TRTLLM_VERSION} ===" && \
    pip install --no-cache-dir \
        --index-url https://pypi.nvidia.com/ \
        --extra-index-url https://pypi.org/simple/ \
        "tensorrt-llm==${TRTLLM_VERSION}" && \
    echo "TensorRT-LLM ${TRTLLM_VERSION} installed"

############################
# Install Dynamo
############################
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ -e "." && \
    echo "Dynamo installed"

############################
# Create tensorrt_llm.metrics stub if needed
############################
RUN python3 -c "import tensorrt_llm.metrics" 2>/dev/null || \
    (echo '"""TensorRT-LLM metrics stub."""\n\
class Metrics:\n\
    def __init__(self): pass\n\
    def report(self, *args, **kwargs): pass\n\
def get_metrics(): return Metrics()\n\
' > /usr/local/lib/python3.12/dist-packages/tensorrt_llm/metrics.py && \
    echo "tensorrt_llm.metrics stub created")

############################
# Validate MPI works without libevent conflict
############################
RUN echo "=== MPI Validation ===" && \
    which mpirun && \
    mpirun --version | head -3 && \
    echo "MPI validated"

############################
# Validation
############################
RUN python3 -c "\
import torch, importlib.metadata as im; \
print('PyTorch:', torch.__version__, '- CUDA:', torch.version.cuda); \
print('TensorRT-LLM:', im.version('tensorrt-llm')); \
print('NIXL:', im.version('nixl')); \
print('ai-dynamo:', im.version('ai-dynamo')); \
"

RUN echo "===========================================" && \
    echo "TensorRT-LLM Production Build Complete (FIXED)" && \
    echo "===========================================" && \
    echo "Stack: CUDA 12.8.1 + TRT-LLM 0.20" && \
    echo "DeepGEMM: ENABLED (CUDA 12.8+ native)" && \
    echo "MPI: Amazon OpenMPI (libevent conflict fixed)" && \
    echo ""

WORKDIR /workspace
CMD ["/bin/bash"]
