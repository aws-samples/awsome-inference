# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# TensorRT-LLM Prefill Worker Configuration for Qwen/Qwen2.5-0.5B-Instruct
# This config enables KV cache transfer in disaggregated serving mode

# Backend configuration
backend: pytorch

# Parallelism settings (single GPU per worker)
tensor_parallel_size: 1
pipeline_parallel_size: 1

# Batch and sequence length settings
max_batch_size: 16
max_num_tokens: 4096
max_seq_len: 4096

# Trust remote code for Qwen model
trust_remote_code: true

# KV cache configuration
kv_cache_config:
  free_gpu_memory_fraction: 0.8

# Enable chunked prefill for better throughput
enable_chunked_prefill: true

# Overlap scheduler not supported in prefill-only workers
disable_overlap_scheduler: true

# CRITICAL: KV cache transceiver config for disaggregated serving
# This enables KV cache transfer between prefill and decode workers
cache_transceiver_config:
  backend: DEFAULT
