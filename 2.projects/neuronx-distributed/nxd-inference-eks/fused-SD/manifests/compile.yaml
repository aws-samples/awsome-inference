apiVersion: v1
kind: ConfigMap
metadata:
  name: neuron-compile-scripts
  namespace: neuron-inference
data:
  compile_model.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    # ---- Inputs / defaults ----
    : "${ENABLE_SPECULATIVE:=false}"
    : "${SPECULATION_LENGTH:=7}"
    : "${TP_DEGREE:=32}"
    : "${BATCH_SIZE:=1}"
    : "${MAX_CONTEXT_LEN:=8192}"
    : "${SEQ_LEN:=8192}"

    # Where the downloader put things
    : "${MODEL_DIRNAME:=Llama-3.3-70B-Instruct}"
    : "${DRAFT_DIRNAME:=Llama-3.2-1B-Instruct}"
    : "${MODEL_ROOT:=/shared/model_hub}"

    MODEL_PATH="${MODEL_ROOT}/${MODEL_DIRNAME}"
    DRAFT_MODEL_PATH="${MODEL_ROOT}/${DRAFT_DIRNAME}"

    # Option 1: separate compiled outputs per configuration
    : "${COMPILED_ROOT:=/shared/compiled_models/Llama-3.3-70B}"
    if [[ "${ENABLE_SPECULATIVE}" == "true" ]]; then
      COMPILED_MODEL_PATH="${COMPILED_ROOT}/spec_slen${SPECULATION_LENGTH}_tp${TP_DEGREE}"
    else
      COMPILED_MODEL_PATH="${COMPILED_ROOT}/nospec_tp${TP_DEGREE}"
    fi
    mkdir -p "${COMPILED_MODEL_PATH}"

    # Sanity checks (fail fast if downloads missing)
    [[ -f "${MODEL_PATH}/config.json" ]] || { echo "Missing target model at ${MODEL_PATH}"; exit 2; }
    if [[ "${ENABLE_SPECULATIVE}" == "true" ]]; then
      [[ -f "${DRAFT_MODEL_PATH}/config.json" ]] || { echo "Missing draft model at ${DRAFT_MODEL_PATH}"; exit 2; }
    fi

    echo "Speculative=${ENABLE_SPECULATIVE} SLEN=${SPECULATION_LENGTH} TP=${TP_DEGREE} BS=${BATCH_SIZE} SEQ=${SEQ_LEN}"
    echo "MODEL_PATH=${MODEL_PATH}"
    echo "DRAFT_MODEL_PATH=${DRAFT_MODEL_PATH}"
    echo "COMPILED_MODEL_PATH=${COMPILED_MODEL_PATH}"

    BASE_ARGS=(
      --model-type llama
      --task-type causal-lm
      run
      --model-path "${MODEL_PATH}"
      --compiled-model-path "${COMPILED_MODEL_PATH}"
      --torch-dtype bfloat16
      --start_rank_id 0
      --local_ranks_size "${TP_DEGREE}"
      --tp-degree "${TP_DEGREE}"
      --batch-size "${BATCH_SIZE}"
      --max-context-length "${MAX_CONTEXT_LEN}"
      --seq-len "${SEQ_LEN}"
      --on-device-sampling
      --top-k 1
      --do-sample
      --fused-qkv
      --sequence-parallel-enabled
      --qkv-kernel-enabled
      --attn-kernel-enabled
      --mlp-kernel-enabled
      --cc-pipeline-tiling-factor 1
      --enable-bucketing
      --enable-block-kv-layout
      --pa-num-blocks 2048
      --pa-block-size 32
      --enable-prefix-caching
      --context-encoding-buckets 512 1024 2048 4096 8192
      --token-generation-buckets 512 1024 2048 4096 8192
      --prefix-buckets 512 1024 2048
      --compile-only
      --prompt "What is annapurna labs?"
    )

    LOG=/shared/compile.log
    if [[ "${ENABLE_SPECULATIVE}" == "true" ]]; then
      BASE_ARGS+=( --draft-model-path "${DRAFT_MODEL_PATH}" --enable-fused-speculation --speculation-length "${SPECULATION_LENGTH}" )
      LOG=/shared/compile_speculative.log
    fi

    echo "inference_demo ${BASE_ARGS[*]}"
    inference_demo "${BASE_ARGS[@]}" 2>&1 | tee "${LOG}"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: neuron-model-compilation
  namespace: neuron-inference
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        workload-type: "neuron-inference"
      tolerations:
      - key: aws.amazon.com/neuron
        operator: Exists
        effect: NoSchedule
      containers:
      - name: model-compiler
        image: public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.9.1-neuronx-py310-sdk2.25.0-ubuntu22.04
        command: ["/bin/bash","-lc"]
        args:
        - |
          set -euxo pipefail
          df -h /shared || true
          # flip this to true when you want SD compile:
          : "${ENABLE_SPECULATIVE:=false}"
          export ENABLE_SPECULATIVE
          bash /scripts/compile_model.sh
        env:
        # tune/override as needed:
        - name: ENABLE_SPECULATIVE
          value: "false"    # set "true" for SD compile
        - name: SPECULATION_LENGTH
          value: "7"
        - name: TP_DEGREE
          value: "32"
        - name: MODEL_DIRNAME
          value: "Llama-3.3-70B-Instruct"
        - name: DRAFT_DIRNAME
          value: "Llama-3.2-1B-Instruct"
        - name: COMPILED_ROOT
          value: "/shared/compiled_models/Llama-3.3-70B"
        resources:
          limits:
            aws.amazon.com/neuroncore: "32"
            memory: "300Gi"
            cpu: "64"
          requests:
            aws.amazon.com/neuroncore: "32"
            memory: "300Gi"
            cpu: "64"
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        - name: scripts
          mountPath: /scripts
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: efs-models-pvc
      - name: scripts
        configMap:
          name: neuron-compile-scripts
          defaultMode: 0755
