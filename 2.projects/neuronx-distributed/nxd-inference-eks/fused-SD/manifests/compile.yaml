apiVersion: v1
kind: ConfigMap
metadata:
  name: neuron-compile-scripts
  namespace: nxd-inference
data:
  compile_model.sh: |
    #!/bin/bash
    set -e
    
    # Model paths
    MODEL_PATH="${MODEL_PATH}"
    COMPILED_MODEL_PATH="${COMPILED_MODEL_PATH}"
    
    HF_TOKEN="${HF_TOKEN}"
    HF_MODEL_ID="${HF_MODEL_ID}"
    HF_DRAFT_MODEL_ID="${HF_DRAFT_MODEL_ID}"
    
    
    # TP and batch config
    ENABLE_SPECULATIVE="${ENABLE_SPECULATIVE}"
    TP_DEGREE="${TP_DEGREE}"
    BATCH_SIZE="${BATCH_SIZE}"
    MAX_CONTEXT_LEN="${MAX_CONTEXT_LEN}"
    SEQ_LEN="${SEQ_LEN}"
    SPECULATION_LENGTH="${SPECULATION_LENGTH}"
    
    # Optional: For speculative decoding
    DRAFT_MODEL_PATH="${DRAFT_MODEL_PATH}"
    
    # Neuron configuration - ADD THESE
    NUM_CORES="${NUM_CORES}"
    LNC="${LNC}"
    MAX_CONTEXT_LEN="${MAX_CONTEXT_LEN}"
    SEQ_LEN="${SEQ_LEN}"
    SPECULATION_LENGTH="${SPECULATION_LENGTH}"
    
    
    # Standard compilation (without speculative decoding)
    if [ "$ENABLE_SPECULATIVE" != "true" ]; then
        echo "Compiling standard neuron llama..."
        inference_demo \
        --model-type llama \
        --task-type causal-lm \
        run \
        --model-path $MODEL_PATH \
        --compiled-model-path $COMPILED_MODEL_PATH \
        --torch-dtype bfloat16 \
        --start_rank_id 0 \
        --local_ranks_size $TP_DEGREE \
        --tp-degree $TP_DEGREE \
        --batch-size $BATCH_SIZE \
        --max-context-length $MAX_CONTEXT_LEN \
        --seq-len $SEQ_LEN \
        --on-device-sampling \
        --top-k 1 \
        --do-sample \
        --fused-qkv \
        --sequence-parallel-enabled \
        --qkv-kernel-enabled \
        --attn-kernel-enabled \
        --mlp-kernel-enabled \
        --cc-pipeline-tiling-factor 1 \
        --pad-token-id 2 \
        --enable-bucketing \
        --context-encoding-buckets 2048 4096 8192 12288 \
        --token-generation-buckets 2048 4096 8192 12800 \
        --prompt "What is annapurna labs?" 2>&1 | tee /shared/compile.log
    else
        # Compilation with speculative decoding
        echo "Compiling with speculative decoding..."
        inference_demo \
        --model-type llama \
        --task-type causal-lm \
        run \
        --model-path $MODEL_PATH \
        --compiled-model-path $COMPILED_MODEL_PATH \
        --torch-dtype bfloat16 \
        --start_rank_id 0 \
        --local_ranks_size $TP_DEGREE \
        --tp-degree $TP_DEGREE \
        --batch-size $BATCH_SIZE \
        --max-context-length 12288 \
        --seq-len 12800 \
        --on-device-sampling \
        --top-k 1 \
        --fused-qkv \
        --sequence-parallel-enabled \
        --qkv-kernel-enabled \
        --attn-kernel-enabled \
        --mlp-kernel-enabled \
        --cc-pipeline-tiling-factor 1 \
        --draft-model-path $DRAFT_MODEL_PATH \
        --enable-fused-speculation \
        --speculation-length $SPECULATION_LENGTH \
        --pad-token-id 2 \
        --enable-bucketing \
        --context-encoding-buckets 2048 4096 8192 12288 \
        --token-generation-buckets 2048 4096 8192 12800 \
        --prompt "What is annapurna labs?" 2>&1 | tee /shared/compile_speculative.log
    fi
    
    echo "Compilation completed successfully!"

---
apiVersion: batch/v1
kind: Job
metadata:
  name: neuron-model-compilation
  namespace: neuron-inference
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        workload-type: "neuron-inference"
      tolerations:
      - key: aws.amazon.com/neuron
        operator: Exists
        effect: NoSchedule
      containers:
      - name: model-compiler
        image: public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.7.2-neuronx-py310-sdk2.24.0-ubuntu22.04
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "Setting up Neuron environment..."
          
          # Activate virtual environment
          source ~/aws_neuronx_venv_pytorch_2_5_nxd_inference/bin/activate
          
          # Install vLLM Neuron fork
          cd /shared
          if [ ! -d "upstreaming-to-vllm" ]; then
            git clone -b neuron-2.24-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git
            cd upstreaming-to-vllm
            pip install -r requirements-neuron.txt
            VLLM_TARGET_DEVICE="neuron" pip install -e .
            cd /shared
          fi
          
          # Download models (if not already present)
          if [ ! -d "/shared/models/Llama-3.3-70B-Instruct" ]; then
            echo "Downloading Llama-3.3-70B-Instruct model..."
            mkdir -p /shared/models
            # Note: Requires HuggingFace token for gated model
            huggingface-cli download meta-llama/Llama-3.3-70B-Instruct --local-dir /shared/models/Llama-3.3-70B-Instruct
          fi
          
          # Optional: Download draft model for speculative decoding
          if [ "$ENABLE_SPECULATIVE" = "true" ] && [ ! -d "/shared/models/Llama-3.2-1B-Instruct" ]; then
            echo "Downloading draft model..."
            huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --local-dir /shared/models/Llama-3.2-1B-Instruct
          fi
          
          # Run compilation
          chmod +x /scripts/compile_model.sh
          /scripts/compile_model.sh
          
        resources:
          limits:
            aws.amazon.com/neuroncore: "128"
            memory: "128Gi"
            cpu: "64"
          requests:
            aws.amazon.com/neuroncore: "128"
            memory: "64Gi"
            cpu: "32"
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        - name: compile-scripts
          mountPath: /scripts
        env:
        - name: BATCH_SIZE
          value: "${BATCH_SIZE}"
        - name: ENABLE_SPECULATIVE
          value: "${ENABLE_SPECULATIVE}"
        - name: MODEL_PATH
          value: "${MODEL_PATH}"
        - name: COMPILED_MODEL_PATH
          value: "${COMPILED_MODEL_PATH}"
        - name: DRAFT_MODEL_PATH
          value: "${DRAFT_MODEL_PATH}"
        - name: TP_DEGREE
          value: "${TP_DEGREE}"
        - name: MAX_CONTEXT_LEN
          value: "${MAX_CONTEXT_LEN}"
        - name: SEQ_LEN
          value: "${SEQ_LEN}"
        - name: SPECULATION_LENGTH
          value: "${SPECULATION_LENGTH}"
        - name: HF_TOKEN
          value: "${HF_TOKEN}"  #
        # Set to "true" for speculative decoding
        - name: HF_MODEL_ID
          value: "${HF_MODEL_ID}"
        - name: HF_DRAFT_MODEL_ID
          value: "${HF_DRAFT_MODEL_ID}"  #
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: efs-models-pvc
      - name: compile-scripts
        configMap:
          name: neuron-compile-scripts
          defaultMode: 0755

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-models-pvc
  namespace: ${NAMESPACE}
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 500Gi