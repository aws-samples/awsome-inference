apiVersion: v1
kind: ConfigMap
metadata:
  name: neuron-vllm-scripts
  namespace: neuron-inference
data:
  start_vllm.sh: |
    #!/bin/bash
    set -euo pipefail

    # ---- Set defaults ----
    
    : "${ENABLE_SPECULATIVE:=false}"
    : "${TP_DEGREE:=32}"
    : "${BATCH_SIZE:=1}"
    : "${MAX_CONTEXT_LEN:=8192}"
    : "${SEQ_LEN:=8192}"
    : "${SPECULATION_LENGTH:=7}"
    : "${MODEL_PATH:=/shared/models/Llama-3.3-70B-Instruct}"
    : "${COMPILED_MODEL_PATH:=/shared/compiled/Llama-3.3-70B}"
    : "${DRAFT_MODEL_PATH:=/shared/models/Llama-3.2-1B-Instruct}"
    : "${HF_MODEL_ID:=meta-llama/Llama-3.3-70B-Instruct}"
    : "${HF_DRAFT_MODEL_ID:=meta-llama/Llama-3.2-1B-Instruct}"

    echo "Speculative: ${ENABLE_SPECULATIVE} | TP=${TP_DEGREE} | BS=${BATCH_SIZE} | SEQ=${SEQ_LEN}"

    export VLLM_NEURON_FRAMEWORK="neuronx-distributed-inference"
    export NEURON_COMPILED_ARTIFACTS="$COMPILED_MODEL_PATH"

    if [ "$ENABLE_SPECULATIVE" = "false" ]; then
      echo "Starting vLLM server (standard mode)..."
      VLLM_RPC_TIMEOUT=100000 python -m vllm.entrypoints.openai.api_server \
        --model "$MODEL_PATH" \
        --max-num-seqs "${MAX_NUM_SEQS:-1}" \
        --max-model-len "$SEQ_LEN" \
        --tensor-parallel-size "$TP_DEGREE" \
        --device neuron \
        --use-v2-block-manager \
        --num-gpu-blocks-override 2048 \
        --enable-prefix-caching \
        --block-size 32 \
        --override-neuron-config '{"is_block_kv_layout": true, "is_prefix_caching": true}' \
        --port 8000
    else
      echo "Starting vLLM server (speculative decoding mode)..."
      VLLM_RPC_TIMEOUT=100000 python -m vllm.entrypoints.openai.api_server \
        --model "$MODEL_PATH" \
        --max-num-seqs "${MAX_NUM_SEQS:-1}" \
        --max-model-len "$SEQ_LEN" \
        --tensor-parallel-size "$TP_DEGREE" \
        --device neuron \
        --speculative-max-model-len "$SEQ_LEN" \
        --speculative-model "$DRAFT_MODEL_PATH" \
        --num-speculative-tokens "$SPECULATION_LENGTH" \
        --use-v2-block-manager \
        --num-gpu-blocks-override 2048 \
        --enable-prefix-caching \
        --block-size 32 \
        --override-neuron-config '{"enable_fused_speculation": true, "is_block_kv_layout": true, "is_prefix_caching": true}' \
        --port 8000
    fi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuron-llama-inference
  namespace: neuron-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: neuron-llama-inference
  template:
    metadata:
      labels:
        app: neuron-llama-inference
    spec:
      nodeSelector:
        workload-type: "neuron-inference"
        node.kubernetes.io/instance-type: "trn1.32xlarge"
      tolerations:
      - key: aws.amazon.com/neuron
        operator: Exists
        effect: NoSchedule
      containers:
      - name: vllm-server
        image: public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.9.1-neuronx-py310-sdk2.25.0-ubuntu22.04
        command: ["/bin/bash", "-lc"]
        args:
        - |
          set -euo pipefail
          cd /shared
          exec /scripts/start_vllm.sh
        ports:
        - containerPort: 8000
          name: http
        resources:
          limits:
            aws.amazon.com/neuroncore: "32"   # trn1.32xlarge has 64 NeuronCores total
            memory: "500Gi"
            cpu: "128"
          requests:
            aws.amazon.com/neuroncore: "32"
            memory: "500Gi"
            cpu: "64"
        env:
        - name: MAX_NUM_SEQS
          value: "1"
