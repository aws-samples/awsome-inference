# Hugging Face Configuration
HF_TOKEN=your_huggingface_token_here
HF_MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Llama-70B
HF_DRAFT_MODEL_ID=meta-llama/Llama-3.2-1B-Instruct
MODEL_NAME=llama-3-70B-inst

# Inference Configuration
MAX_MODEL_LEN=12800
SEQ_LEN=12800
MAX_CONTEXT_LEN=12288

# Neuron Configuration
TENSOR_PARALLEL_SIZE=32
TP_DEGREE=32  # Add this - same as TENSOR_PARALLEL_SIZE
NAMESPACE=neuron-inference
BATCH_SIZE=1
MAX_NUM_SEQS=1
ENABLE_SPECULATIVE=false  # Start with false
SPECULATION_LENGTH=7  # Add this

# Paths
MODEL_PATH=/shared/models/Llama-3.3-70B-Instruct
COMPILED_MODEL_PATH=/shared/traced_model/Llama-3.3-70B-Instruct
DRAFT_MODEL_PATH=/shared/models/Llama-3.2-1B-Instruct