# Hugging Face Configuration
HF_TOKEN=your_huggingface_token_here
MODEL_ID=meta-llama/Meta-Llama-3-70B-Instruct
MODEL_NAME=llama-3-70B-inst

# Inference Configuration
MAX_MODEL_LEN=8192 # used by vllm- ensure it is the same as seq len
SEQ_LEN=8192 #used by main.py

MAX_NUM_SEQS=4
TENSOR_PARALLEL_SIZE=8