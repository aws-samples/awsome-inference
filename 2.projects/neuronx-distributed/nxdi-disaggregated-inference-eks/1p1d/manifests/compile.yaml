apiVersion: batch/v1
kind: Job
metadata:
  name: neuron-disaggregated-compilation
  namespace: neuron-inference
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        workload-type: "neuron-inference"
      tolerations:
      - key: aws.amazon.com/neuron
        operator: Exists
        effect: NoSchedule
      containers:
      - name: model-compiler
        image: public.ecr.aws/neuron/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.21.0-ubuntu20.04
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "Setting up Neuron vLLM environment..."
          
          # Activate virtual environment
          source ~/aws_neuronx_venv_pytorch_2_7_nxd_inference/bin/activate

          #source in env vars
          
          # Install Neuron vLLM fork
          cd /shared
          git clone -b neuron-2.24-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git
          cd upstreaming-to-vllm
          pip install -r requirements-neuron.txt
          VLLM_TARGET_DEVICE="neuron" pip install -e .
          cd /shared
          
          # Download model (if not already present)
          if [ ! -d "/shared/models/Llama-3.3-70B-Instruct" ]; then
            echo "Downloading Llama-3.3-70B-Instruct model..."
            mkdir -p /shared/models
            # Note: Requires HuggingFace token for gated model
            huggingface-cli download meta-llama/Llama-3.3-70B-Instruct --local-dir /shared/models/Llama-3.3-70B-Instruct
          fi
          
          # Create compilation script 
          cat > /shared/compile.sh << 'EOL'
          #!/bin/bash
          # Official tutorial compile.sh script
          
          
          export COMPILED_MODEL_PATH="di_traced_model_tp${TP_DEGREE}_b${BATCH_SIZE}/"
          
          inference_demo \
          --model-type llama \
          --task-type causal-lm \
          run \
          --model-path $MODEL_PATH \
          --compiled-model-path $COMPILED_MODEL_PATH \
          --torch-dtype bfloat16 \
          --tp-degree $TP_DEGREE \
          --batch-size $BATCH_SIZE \
          --ctx-batch-size 1 \
          --tkg-batch-size $BATCH_SIZE \
          --is-continuous-batching \
          --max-context-length 8192 \
          --seq-len 8192 \
          --on-device-sampling \
          --fused-qkv \
          --global-topk 256 --dynamic \
          --top-k 50 --top-p 0.9 --temperature 0.7 \
          --do-sample \
          --sequence-parallel-enabled \
          --qkv-kernel-enabled \
          --attn-kernel-enabled \
          --mlp-kernel-enabled \
          --cc-pipeline-tiling-factor 1 \
          --pad-token-id 2 \
          --logical-neuron-cores 2 \
          --context-encoding-buckets 256 512 1024 2048 4096 8192 \
          --token-generation-buckets 512 1024 2048 4096 8192 \
          --apply-seq-ids-mask \
          --enable-bucketing \
          --prompt "test prompt" \
          --save-sharded-checkpoint \
          --attn-block-tkg-nki-kernel-enabled \
          --attn-block-tkg-nki-kernel-cache-update \
          --k-cache-transposed \
          --async-mode \
          --compile-only
          EOL
          
          chmod +x /shared/compile.sh
          
          # Compile for multi-instance (TP=32)
          echo "Compiling model for multi-instance deployment (TP=64)..."
          cd /shared
          ./compile.sh --tp-degree 64 --batch-size 4 --model-path /shared/models/Llama-3.3-70B-Instruct
      
          # Create metadata file
          cat > /shared/compilation_metadata.json << EOL
          {
            "model_name": "meta-llama/Llama-3.3-70B-Instruct",
            "compilation_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "neuron_sdk_version": "2.24.0",
            "vllm_version": "0.7.2",
            "configurations": {
              "multi_instance": {
                "tp_degree": 64,
                "batch_size": 4,
                "compiled_path": "di_traced_model_tp64_b4/"
              },
              "single_instance": {
                "tp_degree": 32,
                "batch_size": 4,
                "compiled_path": "di_traced_model_tp32_b4/"
              }
            }
          }
          EOL
          
          echo "Model compilation completed successfully!"
          
        resources:
          limits:
            aws.amazon.com/neuroncore: "32"
            memory: "64Gi"
            cpu: "32"
          requests:
            aws.amazon.com/neuroncore: "32"
            memory: "32Gi"
            cpu: "16"
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        env:
        - name: NEURON_RT_NUM_CORES
          value: "32"
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: efs-models-pvc

