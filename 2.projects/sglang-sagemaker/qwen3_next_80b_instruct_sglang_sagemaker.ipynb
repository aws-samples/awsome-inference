{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db653c6",
   "metadata": {},
   "source": [
    "# Qwen3-Next-80B-A3B-Instruct on SageMaker with SGLang\n",
    "\n",
    "This notebook deploys **[Qwen/Qwen3-Next-80B-A3B-Instruct]** on **Amazon SageMaker** using **SGLang**. It:\n",
    "- Downloads model weights from Hugging Face → uploads to your **S3** bucket\n",
    "- Builds a **SageMaker-compatible SGLang** container (extends upstream `Dockerfile.sagemaker`)\n",
    "- Creates a **real-time endpoint** on a multi-GPU instance (e.g., `ml.p5.48xlarge`)\n",
    "- Exposes an **OpenAI-compatible** API for chat completions\n",
    "\n",
    "> ⚠️ **Heads-up**: Qwen3-Next-80B-A3B is *very* large. Make sure your account has quota for H100 `p5.48xlarge` instances and sufficient EBS/S3 storage and network bandwidth. You’ll also need a Hugging Face token with access to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70c043",
   "metadata": {},
   "source": [
    "## References\n",
    "- Upstream SGLang Dockerfile for SageMaker: `docker/Dockerfile.sagemaker` in the SGLang repo\n",
    "- Example structure based on the **DeepSeek SGLang** SageMaker notebook from the `aws-samples/sagemaker-genai-hosting-examples` repo\n",
    "- Qwen3-Next-80B-A3B-Instruct model card on Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119cc66-46ef-4c5f-b2f6-d04abe059eb0",
   "metadata": {},
   "source": [
    "### About **Qwen3-Next-80B-A3B-Instruct**\n",
    "\n",
    "**Qwen3-Next-80B-A3B-Instruct** is a very large, instruction-tuned multilingual LLM from the Qwen family, designed for high-quality reasoning, tool-use/code assistance, and general chat. As an “Instruct” variant, it’s optimized to follow natural-language directions with guardrails for safer, more helpful outputs.\n",
    "\n",
    "**Highlights**\n",
    "- **Scale & quality:** 80B parameters for strong reasoning and generation quality across broad tasks (analysis, coding, data wrangling, multi-turn chat).\n",
    "- **Instruction tuning (“A3B”):** Aligned for cooperative behavior and concise, on-topic responses.\n",
    "- **Long-context capable:** Supports long prompts/outputs (check the model card & your hardware budget for practical limits).\n",
    "- **Multilingual coverage:** Performs across multiple languages; English-first workflows typically see top quality.\n",
    "- **Enterprise considerations:** Requires multi-GPU nodes (e.g., H100/A100) and fast storage/network for weight loading and high-throughput serving.\n",
    "\n",
    "**Common uses**\n",
    "- Analyst-style reasoning and summarization\n",
    "- Code explanation/refactoring and structured extraction\n",
    "- RAG/chat over private corpora (pair with a vector store + tools)\n",
    "- Function/tool calling in agentic workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e00aa-5581-49f7-88c5-8b513c566652",
   "metadata": {},
   "source": [
    "### Why **SGLang on SageMaker** for Qwen3-Next-80B\n",
    "\n",
    "**SGLang** is a high-throughput LLM serving stack that exposes an **OpenAI-compatible API** and focuses on efficient multi-request scheduling on GPUs. Running it on **Amazon SageMaker** lets you scale and operate the service with managed infrastructure.\n",
    "\n",
    "**Serving benefits with SGLang**\n",
    "- **OpenAI-style API**: Easy drop-in for chat/completions tooling.\n",
    "- **High throughput**: Continuous batching, Radix Attention, & smart scheduling to keep GPUs busy.\n",
    "- **Distributed inference**: Tensor/Data/Expert parallelism flags (e.g., `--tp`, `--dp`, `--ep`) to shard 80B across multiple GPUs.\n",
    "- **Latency controls**: Tunables like `--chunked-prefill-size`, max batch tokens, and KV-cache dtype to trade latency vs. throughput.\n",
    "- **Simple entrypoint**: The repo’s `docker/serve` script launches `sglang.launch_server` on **port 8080** (SageMaker’s expected port).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d93f0",
   "metadata": {},
   "source": [
    "## 0) Prerequisites\n",
    "- AWS credentials with permissions for SageMaker, ECR, S3, CloudWatch\n",
    "- A SageMaker Notebook/Studio instance with **Docker** available (or run the Docker build steps elsewhere and just push to ECR) and configure EBS volume to be big enough to download the model\n",
    "- HF token with model access: https://huggingface.co/settings/tokens\n",
    "- (Optional) VPC/Subnets/Security Groups if deploying privately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a11238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install packages in the notebook kernel\n",
    "%pip install --quiet --upgrade boto3 sagemaker botocore huggingface_hub \"pyyaml<7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb20834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from huggingface_hub import login\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# -----------------------\n",
    "# 1) Configuration\n",
    "# -----------------------\n",
    "AWS_REGION       = os.environ.get(\"AWS_REGION\", \"region\") #update with your region\n",
    "ACCOUNT_ID       = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "S3_BUCKET        = os.environ.get(\"S3_BUCKET\", f\"sagemaker-{AWS_REGION}-{ACCOUNT_ID}\")\n",
    "S3_PREFIX        = os.environ.get(\"S3_PREFIX\", \"models/qwen3-next-80b-a3b-instruct\")\n",
    "MODEL_S3_PREFIX = f\"s3://{S3_BUCKET}/{S3_PREFIX}/\"\n",
    "HF_TOKEN         = os.environ.get(\"HF_TOKEN\", \"\")  # or set manually\n",
    "HF_REPO_ID       = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "\n",
    "# Choose a big multi-GPU instance\n",
    "INSTANCE_TYPE    = os.environ.get(\"INSTANCE_TYPE\", \"ml.p5.48xlarge\")  # 8x H100 80GB\n",
    "INSTANCE_COUNT   = int(os.environ.get(\"INSTANCE_COUNT\", \"1\"))\n",
    "\n",
    "# Parallelism and server tuning (tweak for your hardware & budget)\n",
    "TP_SIZE          = int(os.environ.get(\"TP_SIZE\", \"8\"))   # tensor parallel across 8 GPUs on p5.48xlarge\n",
    "DP_SIZE          = int(os.environ.get(\"DP_SIZE\", \"1\"))   # data parallel replicas per instance\n",
    "\n",
    "# ECR image naming\n",
    "ECR_REPO_NAME    = os.environ.get(\"ECR_REPO_NAME\", \"sglang-sagemaker-qwen80b\")\n",
    "ECR_TAG          = os.environ.get(\"ECR_TAG\", \"v1\")\n",
    "ECR_URI          = f\"{ACCOUNT_ID}.dkr.ecr.{AWS_REGION}.amazonaws.com/{ECR_REPO_NAME}:{ECR_TAG}\"\n",
    "\n",
    "# Try to resolve the SageMaker role (falls back to env var if running outside Studio)\n",
    "try:\n",
    "    ROLE = get_execution_role()\n",
    "except Exception:\n",
    "    ROLE = os.environ.get(\"SAGEMAKER_EXECUTION_ROLE\", \"arn:aws:iam::<YOUR-ACCOUNT-ID>:role/<YOUR-SM-ROLE>\")\n",
    "\n",
    "sm_sess = Session()\n",
    "boto3.setup_default_session(region_name=AWS_REGION)\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "\n",
    "print(\"Region: \", AWS_REGION)\n",
    "print(\"Account:\", ACCOUNT_ID)\n",
    "print(\"Role:   \", ROLE)\n",
    "print(\"Bucket: \", S3_BUCKET)\n",
    "print(\"HF Repo:\", HF_REPO_ID)\n",
    "print(\"Model Prefix: \", MODEL_S3_PREFIX)\n",
    "print(\"HF Repo:\", S3_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Ensure S3 bucket exists (no-op if already created)\n",
    "def ensure_bucket(bucket: str, region: str):\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "        print(f\"Bucket {bucket} exists\")\n",
    "    except Exception:\n",
    "        params = {\"Bucket\": bucket}\n",
    "        if region != \"region\": #update with your region\n",
    "            params[\"CreateBucketConfiguration\"] = {\"LocationConstraint\": region}\n",
    "        s3.create_bucket(**params)\n",
    "        print(f\"Created bucket {bucket}\")\n",
    "\n",
    "ensure_bucket(S3_BUCKET, AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Login to Hugging Face (required to download gated weights if applicable)\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"HF_TOKEN is set.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN is empty. If the model requires auth, set HF_TOKEN first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373e532-667c-4adf-aa7e-7505b2c40a6a",
   "metadata": {},
   "source": [
    "## 4) Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint.\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "Store Models in Your Own S3 Bucket\n",
    "\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93125d26-9716-4662-847d-363f43bd7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = HF_REPO_ID\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95232f32-99ad-412f-97cf-abfc6b7432dc",
   "metadata": {},
   "source": [
    "### Upload model files to S3\n",
    "SageMaker AI allows us to provide uncompressed files. Thus, we directly upload the folder that contains model files to s3\n",
    "\n",
    "Note: The default SageMaker bucket follows the naming pattern: sagemaker-{region}-{account-id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ee89-ea89-4c86-bfe1-51b9f229c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = sm_sess.upload_data(path=model_download_path, key_prefix=S3_PREFIX)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09863b5a",
   "metadata": {},
   "source": [
    "## 5) Build and Push a SageMaker-compatible SGLang image\n",
    "\n",
    "We build the **official SGLang SageMaker Dockerfile** \n",
    "\n",
    "1. **Downloads** the model from your S3 location to the container’s local path\n",
    "2. **Launches** `sglang` with your configured TP/DP/context length on port **8080** for SageMaker\n",
    "\n",
    "> If Docker is not available in this environment, run the build steps on your workstation or in CodeBuild and skip to the ECR push and deploy cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583aa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euxo pipefail\n",
    "REPO_DIR=sglang\n",
    "if [ ! -d \"$REPO_DIR\" ]; then\n",
    "  git clone --depth 1 https://github.com/sgl-project/sglang.git \"$REPO_DIR\"\n",
    "fi\n",
    "cd \"$REPO_DIR\"\n",
    "test -f docker/serve\n",
    "docker build -f docker/Dockerfile.sagemaker -t sglang-sagemaker-base:latest docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "AWS_REGION=\"${AWS_REGION:-$(aws configure get region || echo us-west-2)}\"\n",
    "ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"\n",
    "ECR_REPO_NAME=\"${ECR_REPO_NAME:-sglang-sagemaker-qwen80b}\"\n",
    "ECR_TAG=\"${ECR_TAG:-v1}\"\n",
    "\n",
    "IMAGE_URI=\"${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO_NAME}:${ECR_TAG}\"\n",
    "echo \"Pushing image: ${IMAGE_URI}\"\n",
    "\n",
    "# Ensure repo exists\n",
    "aws ecr describe-repositories --repository-names \"${ECR_REPO_NAME}\" --region \"${AWS_REGION}\" >/dev/null 2>&1 || \\\n",
    "  aws ecr create-repository --repository-name \"${ECR_REPO_NAME}\" --region \"${AWS_REGION}\" >/dev/null\n",
    "\n",
    "# Login, tag, push\n",
    "aws ecr get-login-password --region \"${AWS_REGION}\" | \\\n",
    "  docker login --username AWS --password-stdin \"${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com\"\n",
    "\n",
    "docker tag sglang-sagemaker-base:latest \"${IMAGE_URI}\"\n",
    "docker push \"${IMAGE_URI}\"\n",
    "\n",
    "echo \"Pushed ${IMAGE_URI}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c466e2",
   "metadata": {},
   "source": [
    "## 6) Create SageMaker Model, Endpoint Config, and Endpoint\n",
    "\n",
    "We pass environment variables to the container so the entrypoint can **sync S3 → local** and **launch SGLang** automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    # Use ModelDataSource with S3Prefix (no tarball) so SageMaker lays files under /opt/ml/model\n",
    "    model_data={\n",
    "        \"S3DataSource\": {\n",
    "            \"S3Uri\": MODEL_S3_PREFIX,\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        }\n",
    "    },\n",
    "    role=ROLE,\n",
    "    image_uri=ECR_URI,\n",
    "    env={\n",
    "        \"TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "        #\"PORT\": \"8080\",\n",
    "    },\n",
    "    predictor_cls=Predictor,\n",
    "    #sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c727c7-5d86-4d6e-9d05-d26760c7ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- deploy ---\n",
    "INSTANCE_TYPE   = \"ml.p5.48xlarge\"                     # 8x H100 (change as needed)\n",
    "ENDPOINT_NAME   = \"qwen3-next-80b-sglang\" \n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da0ab9",
   "metadata": {},
   "source": [
    "## 7) Test the endpoint (OpenAI-compatible)\n",
    "\n",
    "SGLang exposes an OpenAI-compatible API. SageMaker forwards HTTP to the container on port 8080 at the `/invocations` path.\n",
    "\n",
    "The code below sends a basic chat request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, os\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=AWS_REGION)\n",
    "payload = {\n",
    "    \"model\": \"Qwen3-Next-80B-A3B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Howdy, what is sglang?.\"}\n",
    "    ],\n",
    "    # Add any SGLang-specific params if needed, e.g. temperature, max_tokens\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_tokens\": 128\n",
    "}\n",
    "resp = runtime.invoke_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload).encode(\"utf-8\"),\n",
    ")\n",
    "print(\"StatusCode:\", resp[\"ResponseMetadata\"][\"HTTPStatusCode\"])\n",
    "print(\"Body:\", resp[\"Body\"].read().decode(\"utf-8\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968fa0a6",
   "metadata": {},
   "source": [
    "## 8) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19944cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop and remove the endpoint + config + model\n",
    "# (Be careful! This will delete the endpoint.)\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=AWS_REGION)\n",
    "\n",
    "print(\"Deleting endpoint...\")\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.get_waiter(\"endpoint_deleted\").wait(EndpointName=endpoint_name)\n",
    "print(\"Deleted endpoint:\", endpoint_name)\n",
    "\n",
    "print(\"Deleting endpoint config...\")\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(\"Deleted endpoint config:\", endpoint_config_name)\n",
    "\n",
    "print(\"Deleting model...\")\n",
    "sm.delete_model(ModelName=model_name)\n",
    "print(\"Deleted model:\", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
